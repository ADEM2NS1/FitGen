{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573d9ca-0a8d-4b7a-a086-e62db4cac355",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"Updated notebook with new results\"\n",
    "!git push origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c932113-8fe6-4ef6-900e-42e64ba5f59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Adem Tounsi\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "üöÄ Initializing models and data...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Ask your question (or 'exit'):  i m 24 yers old 183 cm and weight 90 kg give a starter workout\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìö Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "### ELITE SWIMMING WORKOUT_full_text.txt Expands the size of your hands, to work on your pull. Get a size that works for your hand, I wouldn‚Äôt recommend going overboard with the monster sized ones. Buoy: To put in between your legs and to stop kicking, and for rotations. Bread and butter of swim workouts. Chutes: Strap it to your waist and significantly increase the resistance you feel in the water. Must have for sprinters. Med Balls: Like a basketball, but heavy. Provides just the right medicine you need to get your muscles in shape. *All the of the workouts are designed\n",
      "\n",
      "--- Chunk 2 ---\n",
      "### MASTERING SWIMMING_full_text.txt raises your heart rate into an aerobic zone (60 to 70 percent of MHR). After the warm-up, move through the workout from the largest muscle groups (abdominals) to the smaller, more specific ones (arms). Finally, scheduling 10 minutes to relax and stretch your muscles at the conclusion of your session will reduce any soreness you may feel the next day. --- Page 148 Text --- 132 Mastering Swimming Cool Toys for dry-land Training Athletes who do their dry-land training in a gym have the luxury of selecting from a variety of equipment, such as hand weights, full\n",
      "\n",
      "--- Chunk 3 ---\n",
      "### STRENGTH TRAINING FOR FASTER SWIMMING_full_text.txt result of a growth-related posture imbalance in which the spine curves toward the belly. It is a condition that is usually soon outgrown. Children at this stage have an immense amount of energy but limited coordination and very little endurance. The objective of strength training at this stage of development is to build a solid athletic base. Appropriate strength training sessions three to five days a week, for about 15 minutes, should include activities that promote the five areas of general athleticism ‚Äì using the child‚Äôs own resistance only ‚Äì in short periods of\n",
      "\n",
      "üß† Generating answer with LLaMA 3 (Ollama)...\n",
      "\n",
      "üí¨ Answer:\n",
      " Based on the provided context, I'd recommend a starter workout that focuses on building overall athleticism. Since you're 24 years old, 183 cm tall, and weigh 90 kg, here's a starting point:\n",
      "\n",
      "**Warm-up (5 minutes)**\n",
      "\n",
      "* Light swimming to get your heart rate up\n",
      "* Dynamic stretching for your major muscle groups (legs, hips, back, arms, and shoulders)\n",
      "\n",
      "**Strength Training (15-20 minutes)**\n",
      "\n",
      "* Mastering Swimming: Focus on exercises that work multiple muscle groups at once. For example:\n",
      "\t+ Bodyweight squats (10 reps)\n",
      "\t+ Push-ups (10 reps)\n",
      "\t+ Planks (30-second hold)\n",
      "\t+ Lunges (10 reps per leg)\n",
      "\t+\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# === Main Loop ===\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müîç Ask your question (or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Ask your question (or 'exit'):  exit\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import ollama\n",
    "\n",
    "# === Load everything once ===\n",
    "print(\"üöÄ Initializing models and data...\")\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(\"fitgen_vector_db/fitgen_index.faiss\")\n",
    "\n",
    "# Load chunked text\n",
    "with open(\"fitgen_vector_db/fitgen_chunks.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunk_lines = [line.strip() for line in f.read().split(\"\\n\\n\") if line.strip()]\n",
    "\n",
    "# Load SentenceTransformer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\").to(device)\n",
    "if device == \"cuda\":\n",
    "    model = model.half()  # Use float16 on GPU for faster inference\n",
    "\n",
    "# === Utility: truncate each chunk ===\n",
    "def truncate(text, max_tokens=100):\n",
    "    return ' '.join(text.split()[:max_tokens])\n",
    "\n",
    "# === Main Loop ===\n",
    "while True:\n",
    "    query = input(\"\\nüîç Ask your question (or 'exit'): \").strip()\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "\n",
    "    # Encode query\n",
    "    query_vec = np.array(model.encode([query], convert_to_numpy=True), dtype='float32')\n",
    "\n",
    "    # Search in FAISS\n",
    "    k = 3\n",
    "    _, I = index.search(query_vec, k)\n",
    "\n",
    "    # Get and truncate top chunks\n",
    "    top_chunks = [truncate(chunk_lines[i], 100) for i in I[0]]\n",
    "\n",
    "    print(\"\\nüìö Top Retrieved Chunks:\")\n",
    "    for i, chunk in enumerate(top_chunks, 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\\n{chunk}\")\n",
    "\n",
    "    # Build prompt for RAG\n",
    "    context = \"\\n\\n\".join(top_chunks)\n",
    "    prompt = f\"\"\"Use the following context to answer the user's question concisely and clearly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    print(\"\\nüß† Generating answer with LLaMA 3 (Ollama)...\")\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model='llama3',  # Faster quantized model\n",
    "            messages=[\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ],\n",
    "            options={\n",
    "                'temperature': 0.5,\n",
    "                'num_predict': 150\n",
    "            }\n",
    "        )\n",
    "        answer = response['message']['content'].strip()\n",
    "        print(\"\\nüí¨ Answer:\\n\", answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Ollama error: {e}\")\n",
    "        print(\"Ensure Ollama is running and llama3:8b-q4 is pulled with 'ollama run llama3:8b-q4'.\")\n",
    "\n",
    "print(\"üëã Session ended.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d895615e-4cfa-4fdf-b492-89f1a907cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"fa1509060a01b6917baaa50c6a09c4f63c385044e57153b6402599517b6d135d\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689d782e-e585-474f-a5e8-cf1bffe43157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading FAISS index and chunks...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Ask your question (or 'exit'):  how to recover faster\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Generating with DeepSeek-V3 from Together.ai...\n",
      "\n",
      "üí¨ Answer:\n",
      " To recover faster after swimming sessions, focus on these key strategies from the provided context:\n",
      "\n",
      "1. **Hydration & Fueling**: Sip water between sessions to avoid dehydration and prevent complete fuel depletion, which slows recovery.  \n",
      "2. **Balanced Training**: Mix hard work with adequate recovery to avoid overtraining and maintain energy levels.  \n",
      "3. **Hydrotherapy**: Use cold-water immersion, contrast baths (alternating hot/cold water), or low-intensity swimming to reduce muscle fatigue.  \n",
      "4. **Rest**: Prioritize sleep, including short naps (30‚Äì60 minutes), if possible.  \n",
      "5. **Pacing**: Avoid rushing anaerobic training‚Äîprogress gradually (\"make haste slowly\") to prevent burnout.  \n",
      "6. **Relaxed\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from together import Together\n",
    "import os\n",
    "\n",
    "# === üîë API Key (make sure it's set in environment) ===\n",
    "# export TOGETHER_API_KEY=your_key_here\n",
    "client = Together()  # Reads from os.environ[\"TOGETHER_API_KEY\"]\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-V3\"\n",
    "K = 10\n",
    "MAX_TOKENS = 150\n",
    "CHUNK_TRUNCATE_TOKENS = 100\n",
    "\n",
    "# === üöÄ Load FAISS index and chunks ===\n",
    "print(\"üîÑ Loading FAISS index and chunks...\")\n",
    "index = faiss.read_index(\"fitgen_vector_db/fitgen_index.faiss\")\n",
    "with open(\"fitgen_vector_db/fitgen_chunks.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunk_lines = [line.strip() for line in f.read().split(\"\\n\\n\") if line.strip()]\n",
    "\n",
    "# === üî† Load SentenceTransformer for embedding ===\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\").to(device)\n",
    "if device == \"cuda\":\n",
    "    model = model.half()\n",
    "\n",
    "def truncate(text, max_tokens=100):\n",
    "    return ' '.join(text.split()[:max_tokens])\n",
    "\n",
    "# === üß† RAG Loop ===\n",
    "while True:\n",
    "    query = input(\"\\nüîç Ask your question (or 'exit'): \").strip()\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Encode and search\n",
    "    query_vec = np.array(model.encode([query], convert_to_numpy=True), dtype='float32')\n",
    "    _, I = index.search(query_vec, k=K)\n",
    "    top_chunks = [truncate(chunk_lines[i], CHUNK_TRUNCATE_TOKENS) for i in I[0]]\n",
    "\n",
    "    # Build prompt\n",
    "    context = \"\\n\\n\".join(top_chunks)\n",
    "    prompt = f\"\"\"You are a helpful assistant.\n",
    "\n",
    "Use the following context to answer the user's question concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    print(\"\\nüß† Generating with DeepSeek-V3 from Together.ai...\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip()\n",
    "        print(\"\\nüí¨ Answer:\\n\", answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error from Together API: {e}\")\n",
    "\n",
    "print(\"üëã Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
