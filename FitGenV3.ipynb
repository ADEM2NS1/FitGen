{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e67ca82-38df-4884-a97b-5ce2958cb9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitialized existing Git repository in C:/Users/Adem Tounsi/Desktop/FitGen/.git/\n"
     ]
    }
   ],
   "source": [
    "!git remote add origin https://github.com/ADEM2NS1/FitGen.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f53ef28-2c95-4a8c-9024-8a48bd292239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.176.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: protobuf in c:\\programdata\\anaconda3\\lib\\site-packages (from google-generativeai) (4.25.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (4.14.1)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.0)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.73.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.5/1.3 MB 1.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.8/1.3 MB 1.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.8/1.3 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.3 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 973.7 kB/s eta 0:00:00\n",
      "Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading google_api_python_client-2.176.0-py3-none-any.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.7 MB 1.3 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 1.0/13.7 MB 1.3 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.3/13.7 MB 1.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.6/13.7 MB 1.3 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 1.8/13.7 MB 1.3 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.1/13.7 MB 1.4 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.4/13.7 MB 1.3 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.6/13.7 MB 1.3 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 2.9/13.7 MB 1.3 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 3.1/13.7 MB 1.4 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 3.7/13.7 MB 1.4 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.9/13.7 MB 1.4 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 4.2/13.7 MB 1.4 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 4.7/13.7 MB 1.5 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 5.0/13.7 MB 1.5 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 5.5/13.7 MB 1.5 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 5.8/13.7 MB 1.6 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 6.3/13.7 MB 1.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 6.8/13.7 MB 1.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 7.1/13.7 MB 1.6 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.6/13.7 MB 1.6 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.9/13.7 MB 1.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 8.4/13.7 MB 1.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 8.9/13.7 MB 1.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 9.2/13.7 MB 1.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 9.7/13.7 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 10.2/13.7 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.5/13.7 MB 1.8 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 11.0/13.7 MB 1.8 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.5/13.7 MB 1.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.8/13.7 MB 1.8 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.3/13.7 MB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.8/13.7 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.4/13.7 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading grpcio-1.73.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.8/4.3 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 1.3/4.3 MB 2.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 1.8/4.3 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 2.4/4.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.6/4.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 3.1/4.3 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.7/4.3 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 4.2/4.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: uritemplate, rsa, protobuf, httplib2, grpcio, proto-plus, googleapis-common-protos, google-auth, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.71.0\n",
      "    Uninstalling grpcio-1.71.0:\n",
      "      Successfully uninstalled grpcio-1.71.0\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.25.1 google-api-python-client-2.176.0 google-auth-2.40.3 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.73.1 grpcio-status-1.71.2 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.5 rsa-4.9.1 uritemplate-4.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\n",
      "streamlit 1.37.1 requires rich<14,>=10.14.0, but you have rich 14.0.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7fed8-e193-4126-9f2f-f00e0e593dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ddc6667-a9c1-4d5b-a14a-765fa878ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBvGfBCJhvd5MJCSnY6XQIoKVAj3qEN0UY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5168bc3b-1e43-4512-b4ca-e324262aef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIzaSyDOqf1EgY9rnGGOtSB4SQlS1DC1NWTbt8Q\n",
    "import os \n",
    "os.environ[\"GOOGLE_API_KEY2\"] = \"AIzaSyC2kcWQo7e6uPPa2KHw4EW6fACgzIXh5sc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d22811cb-7094-40b9-8caa-672f76b3e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY3\"] = \"AIzaSyDOqf1EgY9rnGGOtSB4SQlS1DC1NWTbt8Q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cadc6d6c-8d50-4c85-9c63-a72ef3e87eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Sending PDF 'C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\\ELITE SWIMMING WORKOUT.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'extracted_gemini_text.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import io\n",
    "\n",
    "# --- Configure your Gemini API key ---\n",
    "# IMPORTANT: Your API key should be loaded from an environment variable, not hardcoded.\n",
    "# The line below is trying to use a hardcoded string as a variable name, which is incorrect.\n",
    "# Correct way is: genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "# Make sure you have set the environment variable GOOGLE_API_KEY before running this script.\n",
    "# For example, in your terminal before running Python:\n",
    "# export GOOGLE_API_KEY=\"AIzaSyBvGfBCJhvd5MJCSnY6XQIoKVAj3qEN0UY\" (Linux/macOS)\n",
    "# set GOOGLE_API_KEY=\"AIzaSyBvGfBCJhvd5MJCSnY6XQIoKVAj3qEN0UY\" (Windows CMD)\n",
    "# $env:GOOGLE_API_KEY=\"AIzaSyBvGfBCJhvd5MJCSnY6XQIoKVAj3qEN0UY\" (Windows PowerShell)\n",
    "\n",
    "# Corrected API Key configuration:\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "def extract_text_from_pdf_with_gemini_v2(pdf_path: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Extracts text content from a PDF using Gemini and saves it to a text file.\n",
    "    Uses genai.upload_file() for robust file handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest') # Or 'gemini-1.5-pro-latest'\n",
    "\n",
    "        # Upload the PDF file to Gemini's temporary storage\n",
    "        # This returns a File object that can be passed to generate_content\n",
    "        pdf_file = genai.upload_file(path=pdf_path, display_name=os.path.basename(pdf_path))\n",
    "\n",
    "        # Define the prompt to extract all text\n",
    "        prompt_parts = [\n",
    "            pdf_file, # Pass the uploaded File object directly\n",
    "            \"Extract all readable text content from this PDF document. Present it as continuous text, preserving as much original formatting like line breaks and spacing as possible.\",\n",
    "        ]\n",
    "\n",
    "        print(f\"üîÑ Sending PDF '{pdf_path}' to Gemini for text extraction...\")\n",
    "        response = model.generate_content(prompt_parts)\n",
    "\n",
    "        extracted_text = response.text\n",
    "\n",
    "        # Save the extracted text to a file\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(extracted_text)\n",
    "\n",
    "        print(f\"‚úÖ Text extracted and saved to '{output_file}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting text with Gemini: {e}\")\n",
    "        print(\"Please ensure your GOOGLE_API_KEY environment variable is set correctly and the PDF path is valid.\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_document_path = r\"C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\\contemporary swim start research.pdf\"\n",
    "    output_text_file = \"extracted_gemini_text.txt\"\n",
    "\n",
    "    extract_text_from_pdf_with_gemini_v2(pdf_document_path, output_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "637ac722-e495-4f4c-86a1-92497c1f5e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output folder: C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\\extracted_texts\n",
      "üîÑ Sending PDF '100 MORE SWIMMING DRILLS.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to '100 MORE SWIMMING DRILLS.txt'\n",
      "üîÑ Sending PDF 'ADAPTED AQUATICS PROGRAMMING.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'ADAPTED AQUATICS PROGRAMMING.txt'\n",
      "üîÑ Sending PDF 'BREAKTHROUGH SWIMMING.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'BREAKTHROUGH SWIMMING.txt'\n",
      "üîÑ Sending PDF 'Complete Guide to PRIMARY SWIMMING.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'Complete Guide to PRIMARY SWIMMING.txt'\n",
      "üîÑ Sending PDF 'contemporary swim start research.pdf' to Gemini for text extraction...\n",
      "‚ùå Error extracting text from 'contemporary swim start research.pdf' with Gemini: Timeout of 600.0s exceeded, last exception: 503 The model is overloaded. Please try again later.\n",
      "üîÑ Sending PDF 'ELITE SWIMMING WORKOUT.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'ELITE SWIMMING WORKOUT.txt'\n",
      "üîÑ Sending PDF 'Handbook of Sports Medicine and Science Swimming.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'Handbook of Sports Medicine and Science Swimming.txt'\n",
      "üîÑ Sending PDF 'INTERNATIONAL LIFEGUARD Training Program Manual.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'INTERNATIONAL LIFEGUARD Training Program Manual.txt'\n",
      "üîÑ Sending PDF 'learn to Swim.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'learn to Swim.txt'\n",
      "üîÑ Sending PDF 'Master the Art of Swimming.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'Master the Art of Swimming.txt'\n",
      "üîÑ Sending PDF 'MASTERING SWIMMING.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'MASTERING SWIMMING.txt'\n",
      "üîÑ Sending PDF 'MASTERS SWIMMING A MANUAL.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'MASTERS SWIMMING A MANUAL.txt'\n",
      "üîÑ Sending PDF 'OPEN WATER SWIMMHNG MANUAL.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'OPEN WATER SWIMMHNG MANUAL.txt'\n",
      "üîÑ Sending PDF 'RULEBOOK.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'RULEBOOK.txt'\n",
      "üîÑ Sending PDF 'Science of SWIMMING FASTER.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'Science of SWIMMING FASTER.txt'\n",
      "üîÑ Sending PDF 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf' to Gemini for text extraction...\n",
      "‚ùå Error extracting text from 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf' with Gemini: Invalid operation: The `response.parts` quick accessor requires a single candidate, but but `response.candidates` is empty.\n",
      "This appears to be caused by a blocked prompt, see `response.prompt_feedback`: block_reason: PROHIBITED_CONTENT\n",
      "\n",
      "üîÑ Sending PDF 'SWIM BETTER SWIM FASTER.pdf' to Gemini for text extraction...\n",
      "‚ùå Error extracting text from 'SWIM BETTER SWIM FASTER.pdf' with Gemini: Invalid operation: The `response.parts` quick accessor requires a single candidate, but but `response.candidates` is empty.\n",
      "This appears to be caused by a blocked prompt, see `response.prompt_feedback`: block_reason: PROHIBITED_CONTENT\n",
      "\n",
      "üîÑ Sending PDF 'SWIM LIKE A PRO.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'SWIM LIKE A PRO.txt'\n",
      "üîÑ Sending PDF 'Swim Smooth.pdf' to Gemini for text extraction...\n",
      "‚ùå Error extracting text from 'Swim Smooth.pdf' with Gemini: Invalid operation: The `response.parts` quick accessor requires a single candidate, but but `response.candidates` is empty.\n",
      "This appears to be caused by a blocked prompt, see `response.prompt_feedback`: block_reason: PROHIBITED_CONTENT\n",
      "\n",
      "üîÑ Sending PDF 'SWIMMING A TRAINING PROGRAM.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'SWIMMING A TRAINING PROGRAM.txt'\n",
      "üîÑ Sending PDF 'SWIMMING Anatomy.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'SWIMMING Anatomy.txt'\n",
      "üîÑ Sending PDF 'SWIMMING FOR TOTAL FITNESS.pdf' to Gemini for text extraction...\n",
      "‚úÖ Text extracted and saved to 'SWIMMING FOR TOTAL FITNESS.txt'\n",
      "üîÑ Sending PDF 'Swimming pool technology.pdf' to Gemini for text extraction...\n",
      "‚ùå Error extracting text from 'Swimming pool technology.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 18\n",
      "}\n",
      "]\n",
      "üîÑ Sending PDF 'swimming science.pdf' to Gemini for text extraction...\n",
      "‚ùå Error extracting text from 'swimming science.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "]\n",
      "üîÑ Sending PDF 'Swimming STEPS TO SUCCESS.pdf' to Gemini for text extraction...\n",
      "‚ùå Error extracting text from 'Swimming STEPS TO SUCCESS.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 47\n",
      "}\n",
      "]\n",
      "üîÑ Sending PDF 'SWIMMING.pdf' to Gemini for text extraction...\n",
      "‚ùå Error extracting text from 'SWIMMING.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]\n",
      "üîÑ Sending PDF 'Technique Swim Workouts.pdf' to Gemini for text extraction...\n",
      "‚ùå Error extracting text from 'Technique Swim Workouts.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 1\n",
      "}\n",
      "]\n",
      "üîÑ Sending PDF 'THE 100 BEST SWIMMING DRILLS.pdf' to Gemini for text extraction...\n",
      "‚ùå Error extracting text from 'THE 100 BEST SWIMMING DRILLS.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 7\n",
      "}\n",
      "]\n",
      "üîÑ Sending PDF 'The Swim Coaching Bible.pdf' to Gemini for text extraction...\n",
      "‚ùå Error extracting text from 'The Swim Coaching Bible.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "]\n",
      "üîÑ Sending PDF 'The SWIMMING Drill Book.pdf' to Gemini for text extraction...\n",
      "‚ùå Error extracting text from 'The SWIMMING Drill Book.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 3\n",
      "}\n",
      "]\n",
      "\n",
      "üèÅ All PDF processing attempts completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import time # For adding a small delay to avoid hitting rate limits too quickly\n",
    "\n",
    "# --- Configure your Gemini API key ---\n",
    "# It's crucial that GOOGLE_API_KEY is set in your environment variables.\n",
    "# Example for setting in terminal (Windows CMD):\n",
    "# set GOOGLE_API_KEY=\"YOUR_API_KEY_HERE\"\n",
    "# python your_script_name.py\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Define the folder containing your PDF files\n",
    "PDF_FOLDER = r\"C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\"\n",
    "# Define the folder where extracted text files will be saved\n",
    "OUTPUT_TEXT_FOLDER = os.path.join(PDF_FOLDER, \"extracted_texts\")\n",
    "\n",
    "def extract_text_from_pdf_with_gemini(pdf_path: str, output_txt_path: str):\n",
    "    \"\"\"\n",
    "    Extracts text content from a single PDF using Gemini and saves it to a text file.\n",
    "    Handles both native text and OCR for image-based text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest') # Using Flash for speed\n",
    "\n",
    "        # Upload the PDF file to Gemini's temporary storage\n",
    "        # This returns a File object that can be passed to generate_content\n",
    "        pdf_file = genai.upload_file(path=pdf_path, display_name=os.path.basename(pdf_path))\n",
    "\n",
    "        # Define the prompt to extract all text\n",
    "        prompt_parts = [\n",
    "            pdf_file, # Pass the uploaded File object directly\n",
    "            \"Extract all readable text content from this PDF document. Present it as continuous text, preserving as much original formatting like line breaks and spacing as possible. Include all text from tables if present.\",\n",
    "        ]\n",
    "\n",
    "        print(f\"üîÑ Sending PDF '{os.path.basename(pdf_path)}' to Gemini for text extraction...\")\n",
    "        response = model.generate_content(prompt_parts)\n",
    "\n",
    "        extracted_text = response.text\n",
    "\n",
    "        # Save the extracted text to a file\n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(extracted_text)\n",
    "\n",
    "        print(f\"‚úÖ Text extracted and saved to '{os.path.basename(output_txt_path)}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting text from '{os.path.basename(pdf_path)}' with Gemini: {e}\")\n",
    "        # Common errors: API key not set, quota exceeded, invalid PDF format, network issues.\n",
    "    finally:\n",
    "        # It's good practice to delete the temporary uploaded file to free up storage\n",
    "        # However, be aware of Google's File Manager policy regarding retention.\n",
    "        # For small scripts, often not strictly necessary as files expire automatically.\n",
    "        # If you face \"file not found\" issues in subsequent runs for the same file,\n",
    "        # it might be due to a previous upload still being processed or deleted.\n",
    "        # For this loop, we'll keep it simple and rely on automatic expiry for now.\n",
    "        pass\n",
    "\n",
    "\n",
    "# --- Main script to process all PDFs in the folder ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(OUTPUT_TEXT_FOLDER, exist_ok=True)\n",
    "    print(f\"Created output folder: {OUTPUT_TEXT_FOLDER}\")\n",
    "\n",
    "    pdf_files_found = False\n",
    "    for filename in os.listdir(PDF_FOLDER):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            pdf_files_found = True\n",
    "            pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "            # Create a corresponding .txt filename\n",
    "            output_txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "            output_txt_path = os.path.join(OUTPUT_TEXT_FOLDER, output_txt_filename)\n",
    "\n",
    "            # Skip if the text file already exists (to avoid re-processing)\n",
    "            if os.path.exists(output_txt_path):\n",
    "                print(f\"‚è© Skipping '{filename}' as text already extracted to '{output_txt_filename}'\")\n",
    "                continue\n",
    "\n",
    "            extract_text_from_pdf_with_gemini(pdf_path, output_txt_path)\n",
    "\n",
    "            # Add a small delay between API calls to avoid hitting rate limits\n",
    "            # Adjust this based on your quota and the number of PDFs\n",
    "            time.sleep(5) # Wait for 5 seconds between each PDF processing\n",
    "\n",
    "    if not pdf_files_found:\n",
    "        print(f\"‚ö†Ô∏è No PDF files found in the folder: {PDF_FOLDER}\")\n",
    "\n",
    "    print(\"\\nüèÅ All PDF processing attempts completed.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ccb8c10-35da-4972-8b2c-c8ecf17e0e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tenacity in c:\\programdata\\anaconda3\\lib\\site-packages (8.2.3)\n"
     ]
    }
   ],
   "source": [
    "pip install tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cff027d-076f-4af8-aeea-7add9f262f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output folder: C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\\extracted_texts\n",
      "‚è© Skipping '100 MORE SWIMMING DRILLS.pdf' as text already extracted to '100 MORE SWIMMING DRILLS.txt'\n",
      "‚è© Skipping 'ADAPTED AQUATICS PROGRAMMING.pdf' as text already extracted to 'ADAPTED AQUATICS PROGRAMMING.txt'\n",
      "‚è© Skipping 'BREAKTHROUGH SWIMMING.pdf' as text already extracted to 'BREAKTHROUGH SWIMMING.txt'\n",
      "‚è© Skipping 'Complete Guide to PRIMARY SWIMMING.pdf' as text already extracted to 'Complete Guide to PRIMARY SWIMMING.txt'\n",
      "‚è© Skipping 'contemporary swim start research.pdf' as text already extracted to 'contemporary swim start research.txt'\n",
      "‚è© Skipping 'ELITE SWIMMING WORKOUT.pdf' as text already extracted to 'ELITE SWIMMING WORKOUT.txt'\n",
      "‚è© Skipping 'Handbook of Sports Medicine and Science Swimming.pdf' as text already extracted to 'Handbook of Sports Medicine and Science Swimming.txt'\n",
      "‚è© Skipping 'INTERNATIONAL LIFEGUARD Training Program Manual.pdf' as text already extracted to 'INTERNATIONAL LIFEGUARD Training Program Manual.txt'\n",
      "‚è© Skipping 'learn to Swim.pdf' as text already extracted to 'learn to Swim.txt'\n",
      "‚è© Skipping 'Master the Art of Swimming.pdf' as text already extracted to 'Master the Art of Swimming.txt'\n",
      "‚è© Skipping 'MASTERING SWIMMING.pdf' as text already extracted to 'MASTERING SWIMMING.txt'\n",
      "‚è© Skipping 'MASTERS SWIMMING A MANUAL.pdf' as text already extracted to 'MASTERS SWIMMING A MANUAL.txt'\n",
      "‚è© Skipping 'OPEN WATER SWIMMHNG MANUAL.pdf' as text already extracted to 'OPEN WATER SWIMMHNG MANUAL.txt'\n",
      "‚è© Skipping 'RULEBOOK.pdf' as text already extracted to 'RULEBOOK.txt'\n",
      "‚è© Skipping 'Science of SWIMMING FASTER.pdf' as text already extracted to 'Science of SWIMMING FASTER.txt'\n",
      "‚è© Skipping 'SWIM LIKE A PRO.pdf' as text already extracted to 'SWIM LIKE A PRO.txt'\n",
      "‚è© Skipping 'SWIMMING A TRAINING PROGRAM.pdf' as text already extracted to 'SWIMMING A TRAINING PROGRAM.txt'\n",
      "‚è© Skipping 'SWIMMING Anatomy.pdf' as text already extracted to 'SWIMMING Anatomy.txt'\n",
      "‚è© Skipping 'SWIMMING FOR TOTAL FITNESS.pdf' as text already extracted to 'SWIMMING FOR TOTAL FITNESS.txt'\n",
      "‚è© Skipping 'Swimming pool technology.pdf' as text already extracted to 'Swimming pool technology.txt'\n",
      "Found 10 new PDF(s) to process.\n",
      "üîÑ Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf'...\n",
      "üîÑ Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf'...\n",
      "üîÑ Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf'...\n",
      "üîÑ Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf'...\n",
      "üîÑ Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf'...\n",
      "üîÑ Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf'...\n",
      "‚ùå Error extracting text from 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf' with Gemini after retries: RetryError[<Future at 0x235299eba70 state=finished raised BlockedPromptException>]\n",
      "üîÑ Attempting extraction for 'SWIM BETTER SWIM FASTER.pdf'...\n",
      "üîÑ Attempting extraction for 'SWIM BETTER SWIM FASTER.pdf'...\n",
      "üîÑ Attempting extraction for 'SWIM BETTER SWIM FASTER.pdf'...\n",
      "üîÑ Attempting extraction for 'SWIM BETTER SWIM FASTER.pdf'...\n",
      "üîÑ Attempting extraction for 'SWIM BETTER SWIM FASTER.pdf'...\n",
      "üîÑ Attempting extraction for 'SWIM BETTER SWIM FASTER.pdf'...\n",
      "‚ùå Error extracting text from 'SWIM BETTER SWIM FASTER.pdf' with Gemini after retries: RetryError[<Future at 0x235296d9d60 state=finished raised BlockedPromptException>]\n",
      "üîÑ Attempting extraction for 'Swim Smooth.pdf'...\n",
      "üîÑ Attempting extraction for 'Swim Smooth.pdf'...\n",
      "üîÑ Attempting extraction for 'Swim Smooth.pdf'...\n",
      "üîÑ Attempting extraction for 'Swim Smooth.pdf'...\n",
      "üîÑ Attempting extraction for 'Swim Smooth.pdf'...\n",
      "üîÑ Attempting extraction for 'Swim Smooth.pdf'...\n",
      "‚ùå Error extracting text from 'Swim Smooth.pdf' with Gemini after retries: RetryError[<Future at 0x23529bdec30 state=finished raised BlockedPromptException>]\n",
      "üîÑ Attempting extraction for 'swimming science.pdf'...\n",
      "üîÑ Attempting extraction for 'swimming science.pdf'...\n",
      "üîÑ Attempting extraction for 'swimming science.pdf'...\n",
      "üîÑ Attempting extraction for 'swimming science.pdf'...\n",
      "üîÑ Attempting extraction for 'swimming science.pdf'...\n",
      "üîÑ Attempting extraction for 'swimming science.pdf'...\n",
      "‚ùå Error extracting text from 'swimming science.pdf' with Gemini after retries: RetryError[<Future at 0x23525c47c20 state=finished raised InvalidArgument>]\n",
      "üîÑ Attempting extraction for 'Swimming STEPS TO SUCCESS.pdf'...\n",
      "üîÑ Attempting extraction for 'Swimming STEPS TO SUCCESS.pdf'...\n",
      "üîÑ Attempting extraction for 'Swimming STEPS TO SUCCESS.pdf'...\n",
      "üîÑ Attempting extraction for 'Swimming STEPS TO SUCCESS.pdf'...\n",
      "üîÑ Attempting extraction for 'Swimming STEPS TO SUCCESS.pdf'...\n",
      "üîÑ Attempting extraction for 'Swimming STEPS TO SUCCESS.pdf'...\n",
      "‚ùå Error extracting text from 'Swimming STEPS TO SUCCESS.pdf' with Gemini after retries: RetryError[<Future at 0x23529a6d970 state=finished raised BlockedPromptException>]\n",
      "üîÑ Attempting extraction for 'SWIMMING.pdf'...\n",
      "‚úÖ Text extracted and saved to 'SWIMMING.txt'\n",
      "üîÑ Attempting extraction for 'Technique Swim Workouts.pdf'...\n",
      "‚úÖ Text extracted and saved to 'Technique Swim Workouts.txt'\n",
      "üîÑ Attempting extraction for 'THE 100 BEST SWIMMING DRILLS.pdf'...\n",
      "‚úÖ Text extracted and saved to 'THE 100 BEST SWIMMING DRILLS.txt'\n",
      "üîÑ Attempting extraction for 'The Swim Coaching Bible.pdf'...\n",
      "üîÑ Attempting extraction for 'The Swim Coaching Bible.pdf'...\n",
      "‚úÖ Text extracted and saved to 'The Swim Coaching Bible.txt'\n",
      "üîÑ Attempting extraction for 'The SWIMMING Drill Book.pdf'...\n",
      "‚úÖ Text extracted and saved to 'The SWIMMING Drill Book.txt'\n",
      "\n",
      "üèÅ All PDF processing attempts completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# --- Configure your Gemini API key ---\n",
    "# It's crucial that GOOGLE_API_KEY is set in your environment variables.\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY3\"))\n",
    "\n",
    "# Define the folder containing your PDF files\n",
    "PDF_FOLDER = r\"C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\"\n",
    "# Define the folder where extracted text files will be saved\n",
    "OUTPUT_TEXT_FOLDER = os.path.join(PDF_FOLDER, \"extracted_texts\")\n",
    "\n",
    "# --- Retry decorator for API calls ---\n",
    "# This will retry on your custom BlockedPromptException and any other general Exception\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60), # Wait 4s, 8s, 16s, 32s, 60s, 60s\n",
    "    stop=stop_after_attempt(6), # Try up to 6 times\n",
    "    # Retry on BlockedPromptException or any general Exception (e.g., connection errors, timeouts, API errors)\n",
    "    retry=retry_if_exception_type(genai.types.BlockedPromptException) |\n",
    "          retry_if_exception_type(Exception) # Catch any error that happens during the generate_content call\n",
    ")\n",
    "def _generate_content_with_retry(model, prompt_parts, pdf_filename):\n",
    "    print(f\"üîÑ Attempting extraction for '{pdf_filename}'...\")\n",
    "    # Add a higher timeout for potentially large PDFs, e.g., 10 minutes (600 seconds)\n",
    "    response = model.generate_content(prompt_parts, request_options={\"timeout\": 600})\n",
    "\n",
    "    # Check for prompt feedback, especially for PROHIBITED_CONTENT\n",
    "    if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
    "        # Raise this specific exception so it's caught by tenacity's retry mechanism\n",
    "        raise genai.types.BlockedPromptException(\n",
    "            f\"Prompt blocked: {response.prompt_feedback.block_reason.name} \"\n",
    "            f\"for file {pdf_filename}. Safety ratings: {response.prompt_feedback.safety_ratings}\"\n",
    "        )\n",
    "    return response\n",
    "\n",
    "def extract_text_from_pdf_with_gemini(pdf_path: str, output_txt_path: str):\n",
    "    \"\"\"\n",
    "    Extracts text content from a single PDF using Gemini and saves it to a text file.\n",
    "    Includes retry logic for API issues.\n",
    "    \"\"\"\n",
    "    pdf_filename = os.path.basename(pdf_path)\n",
    "    # Initialize pdf_file to None so it exists even if genai.upload_file fails\n",
    "    pdf_file = None\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest') # Using Flash for speed\n",
    "\n",
    "        pdf_file = genai.upload_file(path=pdf_path, display_name=pdf_filename)\n",
    "\n",
    "        prompt_parts = [\n",
    "            pdf_file, # Pass the uploaded File object directly\n",
    "            \"Extract all readable text content from this PDF document. Present it as continuous text, preserving as much original formatting like line breaks and spacing as possible. Include all text from tables if present.\",\n",
    "        ]\n",
    "\n",
    "        response = _generate_content_with_retry(model, prompt_parts, pdf_filename)\n",
    "\n",
    "        extracted_text = response.text\n",
    "\n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(extracted_text)\n",
    "\n",
    "        print(f\"‚úÖ Text extracted and saved to '{os.path.basename(output_txt_path)}'\")\n",
    "\n",
    "    except genai.types.BlockedPromptException as e:\n",
    "        print(f\"‚ùå Extraction for '{pdf_filename}' blocked: {e}\")\n",
    "    except Exception as e: # Catch any other unexpected errors\n",
    "        print(f\"‚ùå Error extracting text from '{pdf_filename}' with Gemini after retries: {e}\")\n",
    "    finally:\n",
    "        # Clean up the uploaded file (optional, but good practice if not needed anymore)\n",
    "        try:\n",
    "            if pdf_file and pdf_file.name: # Check if pdf_file object was successfully created and has a name\n",
    "                genai.delete_file(pdf_file.name)\n",
    "                # print(f\"Deleted temporary uploaded file for '{pdf_filename}'.\") # Optional: uncomment for verbose cleanup\n",
    "        except Exception as cleanup_e:\n",
    "            # print(f\"Warning: Could not delete temporary file for '{pdf_filename}': {cleanup_e}\") # Optional: for cleanup errors\n",
    "            pass\n",
    "\n",
    "\n",
    "# --- Main script to process all PDFs in the folder ---\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(OUTPUT_TEXT_FOLDER, exist_ok=True)\n",
    "    print(f\"Created output folder: {OUTPUT_TEXT_FOLDER}\")\n",
    "\n",
    "    pdf_files_to_process = []\n",
    "    for filename in os.listdir(PDF_FOLDER):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "            output_txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "            output_txt_path = os.path.join(OUTPUT_TEXT_FOLDER, output_txt_filename)\n",
    "\n",
    "            if os.path.exists(output_txt_path):\n",
    "                print(f\"‚è© Skipping '{filename}' as text already extracted to '{output_txt_filename}'\")\n",
    "                continue\n",
    "            else:\n",
    "                pdf_files_to_process.append((pdf_path, output_txt_path))\n",
    "\n",
    "    if not pdf_files_to_process:\n",
    "        print(f\"‚ö†Ô∏è No new PDF files found to process in: {PDF_FOLDER}\")\n",
    "    else:\n",
    "        print(f\"Found {len(pdf_files_to_process)} new PDF(s) to process.\")\n",
    "        for pdf_path, output_txt_path in pdf_files_to_process:\n",
    "            extract_text_from_pdf_with_gemini(pdf_path, output_txt_path)\n",
    "            # Add a more generous delay to manage daily/minute quotas\n",
    "            # A 15-second delay allows for ~4 requests per minute. Adjust as needed.\n",
    "            time.sleep(15)\n",
    "\n",
    "    print(\"\\nüèÅ All PDF processing attempts completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a84fe34b-0893-4ce4-aa33-21d0212cad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder: C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\\extracted_texts\n",
      "\n",
      "Attempting to extract text from: STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf\n",
      "üöÄ Uploading 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf' to Gemini...\n",
      "‚úÖ Uploaded 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'. File ID: files/4deatnty3jy1\n",
      "üîÑ Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'...\n",
      "üîÑ Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'...\n",
      "üîÑ Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'...\n",
      "üîÑ Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'...\n",
      "üîÑ Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'...\n",
      "üîÑ Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'...\n",
      "‚ùå Error extracting text from 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf' with Gemini after retries: RetryError[<Future at 0x23529af0140 state=finished raised BlockedPromptException>]\n",
      "üóëÔ∏è Deleted temporary uploaded file for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'.\n",
      "\n",
      "üèÅ Extraction attempt completed for the specified PDF.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# --- Configure your Gemini API key ---\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY3\"))\n",
    "\n",
    "# Define the folder containing your PDF files\n",
    "PDF_FOLDER = r\"C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\"\n",
    "# Define the folder where extracted text files will be saved\n",
    "OUTPUT_TEXT_FOLDER = os.path.join(PDF_FOLDER, \"extracted_texts\")\n",
    "\n",
    "# --- Retry decorator for API calls ---\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(6),\n",
    "    retry=retry_if_exception_type(genai.types.BlockedPromptException) |\n",
    "          retry_if_exception_type(Exception) # Catch any general Exception\n",
    ")\n",
    "def _generate_content_with_retry(model, prompt_parts, pdf_filename):\n",
    "    \"\"\"\n",
    "    Helper function to generate content with retries for API errors.\n",
    "    Raises BlockedPromptException if content is blocked.\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Attempting extraction for '{pdf_filename}'...\")\n",
    "    response = model.generate_content(prompt_parts, request_options={\"timeout\": 600})\n",
    "\n",
    "    if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
    "        raise genai.types.BlockedPromptException(\n",
    "            f\"Prompt blocked: {response.prompt_feedback.block_reason.name} \"\n",
    "            f\"for file {pdf_filename}. Safety ratings: {response.prompt_feedback.safety_ratings}\"\n",
    "        )\n",
    "    return response\n",
    "\n",
    "def extract_text_from_pdf_with_gemini(pdf_path: str, output_txt_path: str):\n",
    "    \"\"\"\n",
    "    Extracts text content from a single PDF using Gemini and saves it to a text file.\n",
    "    Handles API errors and content moderation blocks with retries.\n",
    "    \"\"\"\n",
    "    pdf_filename = os.path.basename(pdf_path)\n",
    "    pdf_file = None # Initialize pdf_file to None\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "        print(f\"üöÄ Uploading '{pdf_filename}' to Gemini...\")\n",
    "        pdf_file = genai.upload_file(path=pdf_path, display_name=pdf_filename)\n",
    "        print(f\"‚úÖ Uploaded '{pdf_filename}'. File ID: {pdf_file.name}\") # For debugging/info\n",
    "\n",
    "        prompt_parts = [\n",
    "            pdf_file,\n",
    "            \"Extract all readable text content from this PDF document. Present it as continuous text, preserving as much original formatting like line breaks and spacing as possible. Include all text from tables if present.\",\n",
    "        ]\n",
    "\n",
    "        response = _generate_content_with_retry(model, prompt_parts, pdf_filename)\n",
    "\n",
    "        extracted_text = response.text\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_txt_path), exist_ok=True) # Ensure output dir exists\n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(extracted_text)\n",
    "\n",
    "        print(f\"‚úÖ Text extracted and saved to '{os.path.basename(output_txt_path)}'\")\n",
    "\n",
    "    except genai.types.BlockedPromptException as e:\n",
    "        print(f\"‚ùå Extraction for '{pdf_filename}' blocked: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting text from '{pdf_filename}' with Gemini after retries: {e}\")\n",
    "    finally:\n",
    "        # Clean up the uploaded file from Google's temporary storage\n",
    "        try:\n",
    "            if pdf_file and pdf_file.name:\n",
    "                genai.delete_file(pdf_file.name)\n",
    "                print(f\"üóëÔ∏è Deleted temporary uploaded file for '{pdf_filename}'.\")\n",
    "        except Exception as cleanup_e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Could not delete temporary file for '{pdf_filename}': {cleanup_e}\")\n",
    "\n",
    "# --- Example usage for 'swimming science_repaired.pdf' ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(OUTPUT_TEXT_FOLDER, exist_ok=True)\n",
    "    print(f\"Output folder: {OUTPUT_TEXT_FOLDER}\")\n",
    "\n",
    "    target_pdf_filename = 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'\n",
    "    pdf_path = os.path.join(PDF_FOLDER, target_pdf_filename)\n",
    "    output_txt_path = os.path.join(OUTPUT_TEXT_FOLDER, os.path.splitext(target_pdf_filename)[0] + \".txt\")\n",
    "\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"‚ùå Error: The file '{target_pdf_filename}' was not found at '{PDF_FOLDER}'.\")\n",
    "    elif os.path.exists(output_txt_path):\n",
    "        print(f\"‚è© Skipping '{target_pdf_filename}' as text already extracted to '{os.path.basename(output_txt_path)}'.\")\n",
    "    else:\n",
    "        print(f\"\\nAttempting to extract text from: {target_pdf_filename}\")\n",
    "        extract_text_from_pdf_with_gemini(pdf_path, output_txt_path)\n",
    "        # Add a small delay if you plan to call this function multiple times in quick succession\n",
    "        time.sleep(5)\n",
    "\n",
    "    print(\"\\nüèÅ Extraction attempt completed for the specified PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "708058e5-10be-4e3d-8b46-092e13e399e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Adem Tounsi\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "‚úÖ Indexed and saved all topic-separated FAISS indexes.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Manual Topic-Tagged Chunking with Embedding and FAISS Storage\n",
    "\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Config\n",
    "input_folder = r\"C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\extracted_texts\"\n",
    "vector_db_folder = \"fitgen_vector_db_topic\"\n",
    "os.makedirs(vector_db_folder, exist_ok=True)\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Define simple keyword-to-topic mapping\n",
    "keyword_topic_map = {\n",
    "    \"shoulder\": \"injury\",\n",
    "    \"pain\": \"injury\",\n",
    "    \"injury\": \"injury\",\n",
    "    \"freestyle\": \"freestyle\",\n",
    "    \"butterfly\": \"butterfly\",\n",
    "    \"breathing\": \"breathing\",\n",
    "    \"kick\": \"freestyle\",\n",
    "    \"warm-up\": \"warmup\",\n",
    "    \"cooldown\": \"warmup\"\n",
    "}\n",
    "\n",
    "# Function to assign topic to chunk\n",
    "def assign_topic(text):\n",
    "    for keyword, topic in keyword_topic_map.items():\n",
    "        if keyword in text.lower():\n",
    "            return topic\n",
    "    return \"general\"\n",
    "\n",
    "# Chunking function\n",
    "\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Storage per topic\n",
    "topic_chunks = {}\n",
    "topic_sources = {}\n",
    "topic_embeddings = {}\n",
    "\n",
    "# Process and assign chunks\n",
    "for filename in os.listdir(input_folder):\n",
    "    if not filename.endswith(\"_full_text.txt\"):\n",
    "        continue\n",
    "    path = os.path.join(input_folder, filename)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_text = f.read()\n",
    "    chunks = chunk_text(full_text)\n",
    "    for chunk in chunks:\n",
    "        topic = assign_topic(chunk)\n",
    "        topic_chunks.setdefault(topic, []).append(chunk)\n",
    "        topic_sources.setdefault(topic, []).append(filename)\n",
    "\n",
    "# Create FAISS indexes per topic\n",
    "for topic, chunks in topic_chunks.items():\n",
    "    embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, f\"{vector_db_folder}/index_{topic}.faiss\")\n",
    "\n",
    "    # Save chunks\n",
    "    with open(f\"{vector_db_folder}/chunks_{topic}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for source, chunk in zip(topic_sources[topic], chunks):\n",
    "            f.write(f\"### {source}\\n{chunk}\\n\\n\")\n",
    "\n",
    "print(\"‚úÖ Indexed and saved all topic-separated FAISS indexes.\")\n",
    "\n",
    "# Step 2: Query Router Based on Keywords\n",
    "\n",
    "def route_query(query):\n",
    "    for keyword, topic in keyword_topic_map.items():\n",
    "        if keyword in query.lower():\n",
    "            return topic\n",
    "    return \"general\"\n",
    "\n",
    "# Step 3: Query FAISS by Topic\n",
    "from sentence_transformers import util\n",
    "\n",
    "def search_topic_index(query, k=5):\n",
    "    topic = route_query(query)\n",
    "    print(f\"üîç Routed to topic: {topic}\")\n",
    "    index_path = f\"{vector_db_folder}/index_{topic}.faiss\"\n",
    "    chunk_path = f\"{vector_db_folder}/chunks_{topic}.txt\"\n",
    "    if not os.path.exists(index_path):\n",
    "        print(\"‚ùå No index found for topic.\")\n",
    "        return\n",
    "\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f.read().split(\"\\n\\n\") if line.strip()]\n",
    "\n",
    "    query_vec = model.encode([query], convert_to_numpy=True)\n",
    "    _, I = index.search(query_vec, k)\n",
    "    print(\"\\nüìö Top Retrieved Chunks:\")\n",
    "    for i in I[0]:\n",
    "        print(f\"\\n--- Chunk ---\\n{lines[i]}\")\n",
    "\n",
    "# üîç Test Example\n",
    "# search_topic_index(\"How do I recover from shoulder injury?\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
