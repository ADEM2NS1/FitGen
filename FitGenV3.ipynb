{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94981ec6-e99f-4314-b6a8-0ba8719d7984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "nothing to commit, working tree clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: 'origin' does not appear to be a git repository\n",
      "fatal: Could not read from remote repository.\n",
      "\n",
      "Please make sure you have the correct access rights\n",
      "and the repository exists.\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"stream lit ui update\"\n",
    "!git push origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10fc4d00-3f5f-4700-896a-bb42ca75fa11",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1910284379.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    git rm -r --cached venv\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git rm -r --cached venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f53ef28-2c95-4a8c-9024-8a48bd292239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (2.25.1)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (2.176.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (5.29.5)\n",
      "Requirement already satisfied: pydantic in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (2.11.7)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from google-generativeai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (4.14.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from pydantic->google-generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from pydantic->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.73.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\adem tounsi\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7fed8-e193-4126-9f2f-f00e0e593dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ddc6667-a9c1-4d5b-a14a-765fa878ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBvGfBCJhvd5MJCSnY6XQIoKVAj3qEN0UY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5168bc3b-1e43-4512-b4ca-e324262aef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIzaSyDOqf1EgY9rnGGOtSB4SQlS1DC1NWTbt8Q\n",
    "import os \n",
    "os.environ[\"GOOGLE_API_KEY2\"] = \"AIzaSyC2kcWQo7e6uPPa2KHw4EW6fACgzIXh5sc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d22811cb-7094-40b9-8caa-672f76b3e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY3\"] = \"AIzaSyDOqf1EgY9rnGGOtSB4SQlS1DC1NWTbt8Q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cadc6d6c-8d50-4c85-9c63-a72ef3e87eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Sending PDF 'C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\\ELITE SWIMMING WORKOUT.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'extracted_gemini_text.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import io\n",
    "\n",
    "# --- Configure your Gemini API key ---\n",
    "# IMPORTANT: Your API key should be loaded from an environment variable, not hardcoded.\n",
    "# The line below is trying to use a hardcoded string as a variable name, which is incorrect.\n",
    "# Correct way is: genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "# Make sure you have set the environment variable GOOGLE_API_KEY before running this script.\n",
    "# For example, in your terminal before running Python:\n",
    "# export GOOGLE_API_KEY=\"AIzaSyBvGfBCJhvd5MJCSnY6XQIoKVAj3qEN0UY\" (Linux/macOS)\n",
    "# set GOOGLE_API_KEY=\"AIzaSyBvGfBCJhvd5MJCSnY6XQIoKVAj3qEN0UY\" (Windows CMD)\n",
    "# $env:GOOGLE_API_KEY=\"AIzaSyBvGfBCJhvd5MJCSnY6XQIoKVAj3qEN0UY\" (Windows PowerShell)\n",
    "\n",
    "# Corrected API Key configuration:\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "def extract_text_from_pdf_with_gemini_v2(pdf_path: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Extracts text content from a PDF using Gemini and saves it to a text file.\n",
    "    Uses genai.upload_file() for robust file handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest') # Or 'gemini-1.5-pro-latest'\n",
    "\n",
    "        # Upload the PDF file to Gemini's temporary storage\n",
    "        # This returns a File object that can be passed to generate_content\n",
    "        pdf_file = genai.upload_file(path=pdf_path, display_name=os.path.basename(pdf_path))\n",
    "\n",
    "        # Define the prompt to extract all text\n",
    "        prompt_parts = [\n",
    "            pdf_file, # Pass the uploaded File object directly\n",
    "            \"Extract all readable text content from this PDF document. Present it as continuous text, preserving as much original formatting like line breaks and spacing as possible.\",\n",
    "        ]\n",
    "\n",
    "        print(f\"🔄 Sending PDF '{pdf_path}' to Gemini for text extraction...\")\n",
    "        response = model.generate_content(prompt_parts)\n",
    "\n",
    "        extracted_text = response.text\n",
    "\n",
    "        # Save the extracted text to a file\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(extracted_text)\n",
    "\n",
    "        print(f\"✅ Text extracted and saved to '{output_file}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting text with Gemini: {e}\")\n",
    "        print(\"Please ensure your GOOGLE_API_KEY environment variable is set correctly and the PDF path is valid.\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_document_path = r\"C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\\contemporary swim start research.pdf\"\n",
    "    output_text_file = \"extracted_gemini_text.txt\"\n",
    "\n",
    "    extract_text_from_pdf_with_gemini_v2(pdf_document_path, output_text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "637ac722-e495-4f4c-86a1-92497c1f5e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output folder: C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\\extracted_texts\n",
      "🔄 Sending PDF '100 MORE SWIMMING DRILLS.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to '100 MORE SWIMMING DRILLS.txt'\n",
      "🔄 Sending PDF 'ADAPTED AQUATICS PROGRAMMING.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'ADAPTED AQUATICS PROGRAMMING.txt'\n",
      "🔄 Sending PDF 'BREAKTHROUGH SWIMMING.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'BREAKTHROUGH SWIMMING.txt'\n",
      "🔄 Sending PDF 'Complete Guide to PRIMARY SWIMMING.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'Complete Guide to PRIMARY SWIMMING.txt'\n",
      "🔄 Sending PDF 'contemporary swim start research.pdf' to Gemini for text extraction...\n",
      "❌ Error extracting text from 'contemporary swim start research.pdf' with Gemini: Timeout of 600.0s exceeded, last exception: 503 The model is overloaded. Please try again later.\n",
      "🔄 Sending PDF 'ELITE SWIMMING WORKOUT.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'ELITE SWIMMING WORKOUT.txt'\n",
      "🔄 Sending PDF 'Handbook of Sports Medicine and Science Swimming.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'Handbook of Sports Medicine and Science Swimming.txt'\n",
      "🔄 Sending PDF 'INTERNATIONAL LIFEGUARD Training Program Manual.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'INTERNATIONAL LIFEGUARD Training Program Manual.txt'\n",
      "🔄 Sending PDF 'learn to Swim.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'learn to Swim.txt'\n",
      "🔄 Sending PDF 'Master the Art of Swimming.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'Master the Art of Swimming.txt'\n",
      "🔄 Sending PDF 'MASTERING SWIMMING.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'MASTERING SWIMMING.txt'\n",
      "🔄 Sending PDF 'MASTERS SWIMMING A MANUAL.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'MASTERS SWIMMING A MANUAL.txt'\n",
      "🔄 Sending PDF 'OPEN WATER SWIMMHNG MANUAL.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'OPEN WATER SWIMMHNG MANUAL.txt'\n",
      "🔄 Sending PDF 'RULEBOOK.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'RULEBOOK.txt'\n",
      "🔄 Sending PDF 'Science of SWIMMING FASTER.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'Science of SWIMMING FASTER.txt'\n",
      "🔄 Sending PDF 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf' to Gemini for text extraction...\n",
      "❌ Error extracting text from 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf' with Gemini: Invalid operation: The `response.parts` quick accessor requires a single candidate, but but `response.candidates` is empty.\n",
      "This appears to be caused by a blocked prompt, see `response.prompt_feedback`: block_reason: PROHIBITED_CONTENT\n",
      "\n",
      "🔄 Sending PDF 'SWIM BETTER SWIM FASTER.pdf' to Gemini for text extraction...\n",
      "❌ Error extracting text from 'SWIM BETTER SWIM FASTER.pdf' with Gemini: Invalid operation: The `response.parts` quick accessor requires a single candidate, but but `response.candidates` is empty.\n",
      "This appears to be caused by a blocked prompt, see `response.prompt_feedback`: block_reason: PROHIBITED_CONTENT\n",
      "\n",
      "🔄 Sending PDF 'SWIM LIKE A PRO.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'SWIM LIKE A PRO.txt'\n",
      "🔄 Sending PDF 'Swim Smooth.pdf' to Gemini for text extraction...\n",
      "❌ Error extracting text from 'Swim Smooth.pdf' with Gemini: Invalid operation: The `response.parts` quick accessor requires a single candidate, but but `response.candidates` is empty.\n",
      "This appears to be caused by a blocked prompt, see `response.prompt_feedback`: block_reason: PROHIBITED_CONTENT\n",
      "\n",
      "🔄 Sending PDF 'SWIMMING A TRAINING PROGRAM.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'SWIMMING A TRAINING PROGRAM.txt'\n",
      "🔄 Sending PDF 'SWIMMING Anatomy.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'SWIMMING Anatomy.txt'\n",
      "🔄 Sending PDF 'SWIMMING FOR TOTAL FITNESS.pdf' to Gemini for text extraction...\n",
      "✅ Text extracted and saved to 'SWIMMING FOR TOTAL FITNESS.txt'\n",
      "🔄 Sending PDF 'Swimming pool technology.pdf' to Gemini for text extraction...\n",
      "❌ Error extracting text from 'Swimming pool technology.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 18\n",
      "}\n",
      "]\n",
      "🔄 Sending PDF 'swimming science.pdf' to Gemini for text extraction...\n",
      "❌ Error extracting text from 'swimming science.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 36\n",
      "}\n",
      "]\n",
      "🔄 Sending PDF 'Swimming STEPS TO SUCCESS.pdf' to Gemini for text extraction...\n",
      "❌ Error extracting text from 'Swimming STEPS TO SUCCESS.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 47\n",
      "}\n",
      "]\n",
      "🔄 Sending PDF 'SWIMMING.pdf' to Gemini for text extraction...\n",
      "❌ Error extracting text from 'SWIMMING.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]\n",
      "🔄 Sending PDF 'Technique Swim Workouts.pdf' to Gemini for text extraction...\n",
      "❌ Error extracting text from 'Technique Swim Workouts.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 1\n",
      "}\n",
      "]\n",
      "🔄 Sending PDF 'THE 100 BEST SWIMMING DRILLS.pdf' to Gemini for text extraction...\n",
      "❌ Error extracting text from 'THE 100 BEST SWIMMING DRILLS.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 7\n",
      "}\n",
      "]\n",
      "🔄 Sending PDF 'The Swim Coaching Bible.pdf' to Gemini for text extraction...\n",
      "❌ Error extracting text from 'The Swim Coaching Bible.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "]\n",
      "🔄 Sending PDF 'The SWIMMING Drill Book.pdf' to Gemini for text extraction...\n",
      "❌ Error extracting text from 'The SWIMMING Drill Book.pdf' with Gemini: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 50\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 3\n",
      "}\n",
      "]\n",
      "\n",
      "🏁 All PDF processing attempts completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import time # For adding a small delay to avoid hitting rate limits too quickly\n",
    "\n",
    "# --- Configure your Gemini API key ---\n",
    "# It's crucial that GOOGLE_API_KEY is set in your environment variables.\n",
    "# Example for setting in terminal (Windows CMD):\n",
    "# set GOOGLE_API_KEY=\"YOUR_API_KEY_HERE\"\n",
    "# python your_script_name.py\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Define the folder containing your PDF files\n",
    "PDF_FOLDER = r\"C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\"\n",
    "# Define the folder where extracted text files will be saved\n",
    "OUTPUT_TEXT_FOLDER = os.path.join(PDF_FOLDER, \"extracted_texts\")\n",
    "\n",
    "def extract_text_from_pdf_with_gemini(pdf_path: str, output_txt_path: str):\n",
    "    \"\"\"\n",
    "    Extracts text content from a single PDF using Gemini and saves it to a text file.\n",
    "    Handles both native text and OCR for image-based text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest') # Using Flash for speed\n",
    "\n",
    "        # Upload the PDF file to Gemini's temporary storage\n",
    "        # This returns a File object that can be passed to generate_content\n",
    "        pdf_file = genai.upload_file(path=pdf_path, display_name=os.path.basename(pdf_path))\n",
    "\n",
    "        # Define the prompt to extract all text\n",
    "        prompt_parts = [\n",
    "            pdf_file, # Pass the uploaded File object directly\n",
    "            \"Extract all readable text content from this PDF document. Present it as continuous text, preserving as much original formatting like line breaks and spacing as possible. Include all text from tables if present.\",\n",
    "        ]\n",
    "\n",
    "        print(f\"🔄 Sending PDF '{os.path.basename(pdf_path)}' to Gemini for text extraction...\")\n",
    "        response = model.generate_content(prompt_parts)\n",
    "\n",
    "        extracted_text = response.text\n",
    "\n",
    "        # Save the extracted text to a file\n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(extracted_text)\n",
    "\n",
    "        print(f\"✅ Text extracted and saved to '{os.path.basename(output_txt_path)}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting text from '{os.path.basename(pdf_path)}' with Gemini: {e}\")\n",
    "        # Common errors: API key not set, quota exceeded, invalid PDF format, network issues.\n",
    "    finally:\n",
    "        # It's good practice to delete the temporary uploaded file to free up storage\n",
    "        # However, be aware of Google's File Manager policy regarding retention.\n",
    "        # For small scripts, often not strictly necessary as files expire automatically.\n",
    "        # If you face \"file not found\" issues in subsequent runs for the same file,\n",
    "        # it might be due to a previous upload still being processed or deleted.\n",
    "        # For this loop, we'll keep it simple and rely on automatic expiry for now.\n",
    "        pass\n",
    "\n",
    "\n",
    "# --- Main script to process all PDFs in the folder ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(OUTPUT_TEXT_FOLDER, exist_ok=True)\n",
    "    print(f\"Created output folder: {OUTPUT_TEXT_FOLDER}\")\n",
    "\n",
    "    pdf_files_found = False\n",
    "    for filename in os.listdir(PDF_FOLDER):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            pdf_files_found = True\n",
    "            pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "            # Create a corresponding .txt filename\n",
    "            output_txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "            output_txt_path = os.path.join(OUTPUT_TEXT_FOLDER, output_txt_filename)\n",
    "\n",
    "            # Skip if the text file already exists (to avoid re-processing)\n",
    "            if os.path.exists(output_txt_path):\n",
    "                print(f\"⏩ Skipping '{filename}' as text already extracted to '{output_txt_filename}'\")\n",
    "                continue\n",
    "\n",
    "            extract_text_from_pdf_with_gemini(pdf_path, output_txt_path)\n",
    "\n",
    "            # Add a small delay between API calls to avoid hitting rate limits\n",
    "            # Adjust this based on your quota and the number of PDFs\n",
    "            time.sleep(5) # Wait for 5 seconds between each PDF processing\n",
    "\n",
    "    if not pdf_files_found:\n",
    "        print(f\"⚠️ No PDF files found in the folder: {PDF_FOLDER}\")\n",
    "\n",
    "    print(\"\\n🏁 All PDF processing attempts completed.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ccb8c10-35da-4972-8b2c-c8ecf17e0e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: tenacity in c:\\programdata\\anaconda3\\lib\\site-packages (8.2.3)\n"
     ]
    }
   ],
   "source": [
    "pip install tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cff027d-076f-4af8-aeea-7add9f262f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created output folder: C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\\extracted_texts\n",
      "⏩ Skipping '100 MORE SWIMMING DRILLS.pdf' as text already extracted to '100 MORE SWIMMING DRILLS.txt'\n",
      "⏩ Skipping 'ADAPTED AQUATICS PROGRAMMING.pdf' as text already extracted to 'ADAPTED AQUATICS PROGRAMMING.txt'\n",
      "⏩ Skipping 'BREAKTHROUGH SWIMMING.pdf' as text already extracted to 'BREAKTHROUGH SWIMMING.txt'\n",
      "⏩ Skipping 'Complete Guide to PRIMARY SWIMMING.pdf' as text already extracted to 'Complete Guide to PRIMARY SWIMMING.txt'\n",
      "⏩ Skipping 'contemporary swim start research.pdf' as text already extracted to 'contemporary swim start research.txt'\n",
      "⏩ Skipping 'ELITE SWIMMING WORKOUT.pdf' as text already extracted to 'ELITE SWIMMING WORKOUT.txt'\n",
      "⏩ Skipping 'Handbook of Sports Medicine and Science Swimming.pdf' as text already extracted to 'Handbook of Sports Medicine and Science Swimming.txt'\n",
      "⏩ Skipping 'INTERNATIONAL LIFEGUARD Training Program Manual.pdf' as text already extracted to 'INTERNATIONAL LIFEGUARD Training Program Manual.txt'\n",
      "⏩ Skipping 'learn to Swim.pdf' as text already extracted to 'learn to Swim.txt'\n",
      "⏩ Skipping 'Master the Art of Swimming.pdf' as text already extracted to 'Master the Art of Swimming.txt'\n",
      "⏩ Skipping 'MASTERING SWIMMING.pdf' as text already extracted to 'MASTERING SWIMMING.txt'\n",
      "⏩ Skipping 'MASTERS SWIMMING A MANUAL.pdf' as text already extracted to 'MASTERS SWIMMING A MANUAL.txt'\n",
      "⏩ Skipping 'OPEN WATER SWIMMHNG MANUAL.pdf' as text already extracted to 'OPEN WATER SWIMMHNG MANUAL.txt'\n",
      "⏩ Skipping 'RULEBOOK.pdf' as text already extracted to 'RULEBOOK.txt'\n",
      "⏩ Skipping 'Science of SWIMMING FASTER.pdf' as text already extracted to 'Science of SWIMMING FASTER.txt'\n",
      "⏩ Skipping 'SWIM LIKE A PRO.pdf' as text already extracted to 'SWIM LIKE A PRO.txt'\n",
      "⏩ Skipping 'SWIMMING A TRAINING PROGRAM.pdf' as text already extracted to 'SWIMMING A TRAINING PROGRAM.txt'\n",
      "⏩ Skipping 'SWIMMING Anatomy.pdf' as text already extracted to 'SWIMMING Anatomy.txt'\n",
      "⏩ Skipping 'SWIMMING FOR TOTAL FITNESS.pdf' as text already extracted to 'SWIMMING FOR TOTAL FITNESS.txt'\n",
      "⏩ Skipping 'Swimming pool technology.pdf' as text already extracted to 'Swimming pool technology.txt'\n",
      "Found 10 new PDF(s) to process.\n",
      "🔄 Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf'...\n",
      "🔄 Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf'...\n",
      "🔄 Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf'...\n",
      "🔄 Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf'...\n",
      "🔄 Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf'...\n",
      "🔄 Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf'...\n",
      "❌ Error extracting text from 'STRENGTH TRAINING FOR FASTER SWIMMING.pdf' with Gemini after retries: RetryError[<Future at 0x235299eba70 state=finished raised BlockedPromptException>]\n",
      "🔄 Attempting extraction for 'SWIM BETTER SWIM FASTER.pdf'...\n",
      "🔄 Attempting extraction for 'SWIM BETTER SWIM FASTER.pdf'...\n",
      "🔄 Attempting extraction for 'SWIM BETTER SWIM FASTER.pdf'...\n",
      "🔄 Attempting extraction for 'SWIM BETTER SWIM FASTER.pdf'...\n",
      "🔄 Attempting extraction for 'SWIM BETTER SWIM FASTER.pdf'...\n",
      "🔄 Attempting extraction for 'SWIM BETTER SWIM FASTER.pdf'...\n",
      "❌ Error extracting text from 'SWIM BETTER SWIM FASTER.pdf' with Gemini after retries: RetryError[<Future at 0x235296d9d60 state=finished raised BlockedPromptException>]\n",
      "🔄 Attempting extraction for 'Swim Smooth.pdf'...\n",
      "🔄 Attempting extraction for 'Swim Smooth.pdf'...\n",
      "🔄 Attempting extraction for 'Swim Smooth.pdf'...\n",
      "🔄 Attempting extraction for 'Swim Smooth.pdf'...\n",
      "🔄 Attempting extraction for 'Swim Smooth.pdf'...\n",
      "🔄 Attempting extraction for 'Swim Smooth.pdf'...\n",
      "❌ Error extracting text from 'Swim Smooth.pdf' with Gemini after retries: RetryError[<Future at 0x23529bdec30 state=finished raised BlockedPromptException>]\n",
      "🔄 Attempting extraction for 'swimming science.pdf'...\n",
      "🔄 Attempting extraction for 'swimming science.pdf'...\n",
      "🔄 Attempting extraction for 'swimming science.pdf'...\n",
      "🔄 Attempting extraction for 'swimming science.pdf'...\n",
      "🔄 Attempting extraction for 'swimming science.pdf'...\n",
      "🔄 Attempting extraction for 'swimming science.pdf'...\n",
      "❌ Error extracting text from 'swimming science.pdf' with Gemini after retries: RetryError[<Future at 0x23525c47c20 state=finished raised InvalidArgument>]\n",
      "🔄 Attempting extraction for 'Swimming STEPS TO SUCCESS.pdf'...\n",
      "🔄 Attempting extraction for 'Swimming STEPS TO SUCCESS.pdf'...\n",
      "🔄 Attempting extraction for 'Swimming STEPS TO SUCCESS.pdf'...\n",
      "🔄 Attempting extraction for 'Swimming STEPS TO SUCCESS.pdf'...\n",
      "🔄 Attempting extraction for 'Swimming STEPS TO SUCCESS.pdf'...\n",
      "🔄 Attempting extraction for 'Swimming STEPS TO SUCCESS.pdf'...\n",
      "❌ Error extracting text from 'Swimming STEPS TO SUCCESS.pdf' with Gemini after retries: RetryError[<Future at 0x23529a6d970 state=finished raised BlockedPromptException>]\n",
      "🔄 Attempting extraction for 'SWIMMING.pdf'...\n",
      "✅ Text extracted and saved to 'SWIMMING.txt'\n",
      "🔄 Attempting extraction for 'Technique Swim Workouts.pdf'...\n",
      "✅ Text extracted and saved to 'Technique Swim Workouts.txt'\n",
      "🔄 Attempting extraction for 'THE 100 BEST SWIMMING DRILLS.pdf'...\n",
      "✅ Text extracted and saved to 'THE 100 BEST SWIMMING DRILLS.txt'\n",
      "🔄 Attempting extraction for 'The Swim Coaching Bible.pdf'...\n",
      "🔄 Attempting extraction for 'The Swim Coaching Bible.pdf'...\n",
      "✅ Text extracted and saved to 'The Swim Coaching Bible.txt'\n",
      "🔄 Attempting extraction for 'The SWIMMING Drill Book.pdf'...\n",
      "✅ Text extracted and saved to 'The SWIMMING Drill Book.txt'\n",
      "\n",
      "🏁 All PDF processing attempts completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# --- Configure your Gemini API key ---\n",
    "# It's crucial that GOOGLE_API_KEY is set in your environment variables.\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY3\"))\n",
    "\n",
    "# Define the folder containing your PDF files\n",
    "PDF_FOLDER = r\"C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\"\n",
    "# Define the folder where extracted text files will be saved\n",
    "OUTPUT_TEXT_FOLDER = os.path.join(PDF_FOLDER, \"extracted_texts\")\n",
    "\n",
    "# --- Retry decorator for API calls ---\n",
    "# This will retry on your custom BlockedPromptException and any other general Exception\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60), # Wait 4s, 8s, 16s, 32s, 60s, 60s\n",
    "    stop=stop_after_attempt(6), # Try up to 6 times\n",
    "    # Retry on BlockedPromptException or any general Exception (e.g., connection errors, timeouts, API errors)\n",
    "    retry=retry_if_exception_type(genai.types.BlockedPromptException) |\n",
    "          retry_if_exception_type(Exception) # Catch any error that happens during the generate_content call\n",
    ")\n",
    "def _generate_content_with_retry(model, prompt_parts, pdf_filename):\n",
    "    print(f\"🔄 Attempting extraction for '{pdf_filename}'...\")\n",
    "    # Add a higher timeout for potentially large PDFs, e.g., 10 minutes (600 seconds)\n",
    "    response = model.generate_content(prompt_parts, request_options={\"timeout\": 600})\n",
    "\n",
    "    # Check for prompt feedback, especially for PROHIBITED_CONTENT\n",
    "    if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
    "        # Raise this specific exception so it's caught by tenacity's retry mechanism\n",
    "        raise genai.types.BlockedPromptException(\n",
    "            f\"Prompt blocked: {response.prompt_feedback.block_reason.name} \"\n",
    "            f\"for file {pdf_filename}. Safety ratings: {response.prompt_feedback.safety_ratings}\"\n",
    "        )\n",
    "    return response\n",
    "\n",
    "def extract_text_from_pdf_with_gemini(pdf_path: str, output_txt_path: str):\n",
    "    \"\"\"\n",
    "    Extracts text content from a single PDF using Gemini and saves it to a text file.\n",
    "    Includes retry logic for API issues.\n",
    "    \"\"\"\n",
    "    pdf_filename = os.path.basename(pdf_path)\n",
    "    # Initialize pdf_file to None so it exists even if genai.upload_file fails\n",
    "    pdf_file = None\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest') # Using Flash for speed\n",
    "\n",
    "        pdf_file = genai.upload_file(path=pdf_path, display_name=pdf_filename)\n",
    "\n",
    "        prompt_parts = [\n",
    "            pdf_file, # Pass the uploaded File object directly\n",
    "            \"Extract all readable text content from this PDF document. Present it as continuous text, preserving as much original formatting like line breaks and spacing as possible. Include all text from tables if present.\",\n",
    "        ]\n",
    "\n",
    "        response = _generate_content_with_retry(model, prompt_parts, pdf_filename)\n",
    "\n",
    "        extracted_text = response.text\n",
    "\n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(extracted_text)\n",
    "\n",
    "        print(f\"✅ Text extracted and saved to '{os.path.basename(output_txt_path)}'\")\n",
    "\n",
    "    except genai.types.BlockedPromptException as e:\n",
    "        print(f\"❌ Extraction for '{pdf_filename}' blocked: {e}\")\n",
    "    except Exception as e: # Catch any other unexpected errors\n",
    "        print(f\"❌ Error extracting text from '{pdf_filename}' with Gemini after retries: {e}\")\n",
    "    finally:\n",
    "        # Clean up the uploaded file (optional, but good practice if not needed anymore)\n",
    "        try:\n",
    "            if pdf_file and pdf_file.name: # Check if pdf_file object was successfully created and has a name\n",
    "                genai.delete_file(pdf_file.name)\n",
    "                # print(f\"Deleted temporary uploaded file for '{pdf_filename}'.\") # Optional: uncomment for verbose cleanup\n",
    "        except Exception as cleanup_e:\n",
    "            # print(f\"Warning: Could not delete temporary file for '{pdf_filename}': {cleanup_e}\") # Optional: for cleanup errors\n",
    "            pass\n",
    "\n",
    "\n",
    "# --- Main script to process all PDFs in the folder ---\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(OUTPUT_TEXT_FOLDER, exist_ok=True)\n",
    "    print(f\"Created output folder: {OUTPUT_TEXT_FOLDER}\")\n",
    "\n",
    "    pdf_files_to_process = []\n",
    "    for filename in os.listdir(PDF_FOLDER):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(PDF_FOLDER, filename)\n",
    "            output_txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
    "            output_txt_path = os.path.join(OUTPUT_TEXT_FOLDER, output_txt_filename)\n",
    "\n",
    "            if os.path.exists(output_txt_path):\n",
    "                print(f\"⏩ Skipping '{filename}' as text already extracted to '{output_txt_filename}'\")\n",
    "                continue\n",
    "            else:\n",
    "                pdf_files_to_process.append((pdf_path, output_txt_path))\n",
    "\n",
    "    if not pdf_files_to_process:\n",
    "        print(f\"⚠️ No new PDF files found to process in: {PDF_FOLDER}\")\n",
    "    else:\n",
    "        print(f\"Found {len(pdf_files_to_process)} new PDF(s) to process.\")\n",
    "        for pdf_path, output_txt_path in pdf_files_to_process:\n",
    "            extract_text_from_pdf_with_gemini(pdf_path, output_txt_path)\n",
    "            # Add a more generous delay to manage daily/minute quotas\n",
    "            # A 15-second delay allows for ~4 requests per minute. Adjust as needed.\n",
    "            time.sleep(15)\n",
    "\n",
    "    print(\"\\n🏁 All PDF processing attempts completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a84fe34b-0893-4ce4-aa33-21d0212cad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder: C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\\extracted_texts\n",
      "\n",
      "Attempting to extract text from: STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf\n",
      "🚀 Uploading 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf' to Gemini...\n",
      "✅ Uploaded 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'. File ID: files/4deatnty3jy1\n",
      "🔄 Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'...\n",
      "🔄 Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'...\n",
      "🔄 Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'...\n",
      "🔄 Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'...\n",
      "🔄 Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'...\n",
      "🔄 Attempting extraction for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'...\n",
      "❌ Error extracting text from 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf' with Gemini after retries: RetryError[<Future at 0x23529af0140 state=finished raised BlockedPromptException>]\n",
      "🗑️ Deleted temporary uploaded file for 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'.\n",
      "\n",
      "🏁 Extraction attempt completed for the specified PDF.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "# --- Configure your Gemini API key ---\n",
    "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY3\"))\n",
    "\n",
    "# Define the folder containing your PDF files\n",
    "PDF_FOLDER = r\"C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\pdf\"\n",
    "# Define the folder where extracted text files will be saved\n",
    "OUTPUT_TEXT_FOLDER = os.path.join(PDF_FOLDER, \"extracted_texts\")\n",
    "\n",
    "# --- Retry decorator for API calls ---\n",
    "@retry(\n",
    "    wait=wait_exponential(multiplier=1, min=4, max=60),\n",
    "    stop=stop_after_attempt(6),\n",
    "    retry=retry_if_exception_type(genai.types.BlockedPromptException) |\n",
    "          retry_if_exception_type(Exception) # Catch any general Exception\n",
    ")\n",
    "def _generate_content_with_retry(model, prompt_parts, pdf_filename):\n",
    "    \"\"\"\n",
    "    Helper function to generate content with retries for API errors.\n",
    "    Raises BlockedPromptException if content is blocked.\n",
    "    \"\"\"\n",
    "    print(f\"🔄 Attempting extraction for '{pdf_filename}'...\")\n",
    "    response = model.generate_content(prompt_parts, request_options={\"timeout\": 600})\n",
    "\n",
    "    if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
    "        raise genai.types.BlockedPromptException(\n",
    "            f\"Prompt blocked: {response.prompt_feedback.block_reason.name} \"\n",
    "            f\"for file {pdf_filename}. Safety ratings: {response.prompt_feedback.safety_ratings}\"\n",
    "        )\n",
    "    return response\n",
    "\n",
    "def extract_text_from_pdf_with_gemini(pdf_path: str, output_txt_path: str):\n",
    "    \"\"\"\n",
    "    Extracts text content from a single PDF using Gemini and saves it to a text file.\n",
    "    Handles API errors and content moderation blocks with retries.\n",
    "    \"\"\"\n",
    "    pdf_filename = os.path.basename(pdf_path)\n",
    "    pdf_file = None # Initialize pdf_file to None\n",
    "\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "        print(f\"🚀 Uploading '{pdf_filename}' to Gemini...\")\n",
    "        pdf_file = genai.upload_file(path=pdf_path, display_name=pdf_filename)\n",
    "        print(f\"✅ Uploaded '{pdf_filename}'. File ID: {pdf_file.name}\") # For debugging/info\n",
    "\n",
    "        prompt_parts = [\n",
    "            pdf_file,\n",
    "            \"Extract all readable text content from this PDF document. Present it as continuous text, preserving as much original formatting like line breaks and spacing as possible. Include all text from tables if present.\",\n",
    "        ]\n",
    "\n",
    "        response = _generate_content_with_retry(model, prompt_parts, pdf_filename)\n",
    "\n",
    "        extracted_text = response.text\n",
    "\n",
    "        os.makedirs(os.path.dirname(output_txt_path), exist_ok=True) # Ensure output dir exists\n",
    "        with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(extracted_text)\n",
    "\n",
    "        print(f\"✅ Text extracted and saved to '{os.path.basename(output_txt_path)}'\")\n",
    "\n",
    "    except genai.types.BlockedPromptException as e:\n",
    "        print(f\"❌ Extraction for '{pdf_filename}' blocked: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error extracting text from '{pdf_filename}' with Gemini after retries: {e}\")\n",
    "    finally:\n",
    "        # Clean up the uploaded file from Google's temporary storage\n",
    "        try:\n",
    "            if pdf_file and pdf_file.name:\n",
    "                genai.delete_file(pdf_file.name)\n",
    "                print(f\"🗑️ Deleted temporary uploaded file for '{pdf_filename}'.\")\n",
    "        except Exception as cleanup_e:\n",
    "            print(f\"⚠️ Warning: Could not delete temporary file for '{pdf_filename}': {cleanup_e}\")\n",
    "\n",
    "# --- Example usage for 'swimming science_repaired.pdf' ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(OUTPUT_TEXT_FOLDER, exist_ok=True)\n",
    "    print(f\"Output folder: {OUTPUT_TEXT_FOLDER}\")\n",
    "\n",
    "    target_pdf_filename = 'STRENGTH TRAINING FOR FASTER SWIMMING_repaired.pdf'\n",
    "    pdf_path = os.path.join(PDF_FOLDER, target_pdf_filename)\n",
    "    output_txt_path = os.path.join(OUTPUT_TEXT_FOLDER, os.path.splitext(target_pdf_filename)[0] + \".txt\")\n",
    "\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"❌ Error: The file '{target_pdf_filename}' was not found at '{PDF_FOLDER}'.\")\n",
    "    elif os.path.exists(output_txt_path):\n",
    "        print(f\"⏩ Skipping '{target_pdf_filename}' as text already extracted to '{os.path.basename(output_txt_path)}'.\")\n",
    "    else:\n",
    "        print(f\"\\nAttempting to extract text from: {target_pdf_filename}\")\n",
    "        extract_text_from_pdf_with_gemini(pdf_path, output_txt_path)\n",
    "        # Add a small delay if you plan to call this function multiple times in quick succession\n",
    "        time.sleep(5)\n",
    "\n",
    "    print(\"\\n🏁 Extraction attempt completed for the specified PDF.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "708058e5-10be-4e3d-8b46-092e13e399e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Adem Tounsi\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "✅ Indexed and saved all topic-separated FAISS indexes.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Manual Topic-Tagged Chunking with Embedding and FAISS Storage\n",
    "\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Config\n",
    "input_folder = r\"C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\extracted_texts\"\n",
    "vector_db_folder = \"fitgen_vector_db_topic\"\n",
    "os.makedirs(vector_db_folder, exist_ok=True)\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Define simple keyword-to-topic mapping\n",
    "keyword_topic_map = {\n",
    "    \"shoulder\": \"injury\",\n",
    "    \"pain\": \"injury\",\n",
    "    \"injury\": \"injury\",\n",
    "    \"freestyle\": \"freestyle\",\n",
    "    \"butterfly\": \"butterfly\",\n",
    "    \"breathing\": \"breathing\",\n",
    "    \"kick\": \"freestyle\",\n",
    "    \"warm-up\": \"warmup\",\n",
    "    \"cooldown\": \"warmup\"\n",
    "}\n",
    "\n",
    "# Function to assign topic to chunk\n",
    "def assign_topic(text):\n",
    "    for keyword, topic in keyword_topic_map.items():\n",
    "        if keyword in text.lower():\n",
    "            return topic\n",
    "    return \"general\"\n",
    "\n",
    "# Chunking function\n",
    "\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Storage per topic\n",
    "topic_chunks = {}\n",
    "topic_sources = {}\n",
    "topic_embeddings = {}\n",
    "\n",
    "# Process and assign chunks\n",
    "for filename in os.listdir(input_folder):\n",
    "    if not filename.endswith(\"_full_text.txt\"):\n",
    "        continue\n",
    "    path = os.path.join(input_folder, filename)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_text = f.read()\n",
    "    chunks = chunk_text(full_text)\n",
    "    for chunk in chunks:\n",
    "        topic = assign_topic(chunk)\n",
    "        topic_chunks.setdefault(topic, []).append(chunk)\n",
    "        topic_sources.setdefault(topic, []).append(filename)\n",
    "\n",
    "# Create FAISS indexes per topic\n",
    "for topic, chunks in topic_chunks.items():\n",
    "    embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, f\"{vector_db_folder}/index_{topic}.faiss\")\n",
    "\n",
    "    # Save chunks\n",
    "    with open(f\"{vector_db_folder}/chunks_{topic}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for source, chunk in zip(topic_sources[topic], chunks):\n",
    "            f.write(f\"### {source}\\n{chunk}\\n\\n\")\n",
    "\n",
    "print(\"✅ Indexed and saved all topic-separated FAISS indexes.\")\n",
    "\n",
    "# Step 2: Query Router Based on Keywords\n",
    "\n",
    "def route_query(query):\n",
    "    for keyword, topic in keyword_topic_map.items():\n",
    "        if keyword in query.lower():\n",
    "            return topic\n",
    "    return \"general\"\n",
    "\n",
    "# Step 3: Query FAISS by Topic\n",
    "from sentence_transformers import util\n",
    "\n",
    "def search_topic_index(query, k=5):\n",
    "    topic = route_query(query)\n",
    "    print(f\"🔍 Routed to topic: {topic}\")\n",
    "    index_path = f\"{vector_db_folder}/index_{topic}.faiss\"\n",
    "    chunk_path = f\"{vector_db_folder}/chunks_{topic}.txt\"\n",
    "    if not os.path.exists(index_path):\n",
    "        print(\"❌ No index found for topic.\")\n",
    "        return\n",
    "\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f.read().split(\"\\n\\n\") if line.strip()]\n",
    "\n",
    "    query_vec = model.encode([query], convert_to_numpy=True)\n",
    "    _, I = index.search(query_vec, k)\n",
    "    print(\"\\n📚 Top Retrieved Chunks:\")\n",
    "    for i in I[0]:\n",
    "        print(f\"\\n--- Chunk ---\\n{lines[i]}\")\n",
    "\n",
    "# 🔍 Test Example\n",
    "# search_topic_index(\"How do I recover from shoulder injury?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3e52e88-bea0-410d-bf74-b5120146223b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Adem Tounsi\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d539ed8b58fa458993e34c741254844e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adem Tounsi\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Adem Tounsi\\.cache\\huggingface\\hub\\models--BAAI--bge-base-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0933b27947462cb645b5732f6efc04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149f0d98464b40e98d884db85f83e35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f9c200cd944915b36818849b8b6244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470f98b0c33540e180d8344c1629c070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2813bbacfd8b4c5ba8aebad0a60df7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140a25ddb8364cd59d5d2aeb085d3ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c79d36d239455889e696072f5d661b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a929c138cf45beb744a4527d35327c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a7bb53e7a64734b5205cd7b5b7d9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aa98aa74a5452a965f3cebf0bd04a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed and saved all multi-topic FAISS indexes.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Manual Multi-Topic Chunking with Embedding and FAISS Storage\n",
    "\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Config\n",
    "input_folder = r\"C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\extracted_texts\"\n",
    "vector_db_folder = \"fitgen_vector_db_topic2\"\n",
    "os.makedirs(vector_db_folder, exist_ok=True)\n",
    "\n",
    "# Use BGE base model\n",
    "model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Expanded keyword-to-topic mapping\n",
    "keyword_topic_map = {\n",
    "    \"shoulder\": \"injury\",\n",
    "    \"pain\": \"injury\",\n",
    "    \"injury\": \"injury\",\n",
    "    \"rehab\": \"injury\",\n",
    "    \"recovery\": \"injury\",\n",
    "    \"freestyle\": \"freestyle\",\n",
    "    \"crawl\": \"freestyle\",\n",
    "    \"kick\": \"freestyle\",\n",
    "    \"butterfly\": \"butterfly\",\n",
    "    \"dolphin\": \"butterfly\",\n",
    "    \"breaststroke\": \"breaststroke\",\n",
    "    \"frog\": \"breaststroke\",\n",
    "    \"breathing\": \"breathing\",\n",
    "    \"inhale\": \"breathing\",\n",
    "    \"exhale\": \"breathing\",\n",
    "    \"warm-up\": \"warmup\",\n",
    "    \"cooldown\": \"warmup\",\n",
    "    \"stretch\": \"warmup\",\n",
    "    \"technique\": \"technique\",\n",
    "    \"form\": \"technique\",\n",
    "    \"endurance\": \"conditioning\",\n",
    "    \"conditioning\": \"conditioning\",\n",
    "    \"strength\": \"conditioning\"\n",
    "}\n",
    "\n",
    "# Function to assign multiple topics to a chunk\n",
    "def assign_topics(text):\n",
    "    topics = set()\n",
    "    for keyword, topic in keyword_topic_map.items():\n",
    "        if keyword in text.lower():\n",
    "            topics.add(topic)\n",
    "    return topics or {\"general\"}\n",
    "\n",
    "# Chunking function\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Storage per topic\n",
    "topic_chunks = {}\n",
    "topic_sources = {}\n",
    "\n",
    "# Process and assign chunks\n",
    "for filename in os.listdir(input_folder):\n",
    "    if not filename.endswith(\"_full_text.txt\"):\n",
    "        continue\n",
    "    path = os.path.join(input_folder, filename)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_text = f.read()\n",
    "    chunks = chunk_text(full_text)\n",
    "    for chunk in chunks:\n",
    "        topics = assign_topics(chunk)\n",
    "        for topic in topics:\n",
    "            topic_chunks.setdefault(topic, []).append(chunk)\n",
    "            topic_sources.setdefault(topic, []).append(filename)\n",
    "\n",
    "# Create FAISS indexes per topic\n",
    "for topic, chunks in topic_chunks.items():\n",
    "    embeddings = model.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, f\"{vector_db_folder}/index_{topic}.faiss\")\n",
    "\n",
    "    # Save chunks\n",
    "    with open(f\"{vector_db_folder}/chunks_{topic}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for source, chunk in zip(topic_sources[topic], chunks):\n",
    "            f.write(f\"### {source}\\n{chunk}\\n\\n\")\n",
    "\n",
    "print(\"✅ Indexed and saved all multi-topic FAISS indexes.\")\n",
    "\n",
    "# Step 2: Query Router Based on Keywords\n",
    "def route_query(query):\n",
    "    for keyword, topic in keyword_topic_map.items():\n",
    "        if keyword in query.lower():\n",
    "            return topic\n",
    "    return \"general\"\n",
    "\n",
    "# Step 3: Query FAISS by Topic\n",
    "from sentence_transformers import util\n",
    "\n",
    "def search_topic_index(query, k=5):\n",
    "    topic = route_query(query)\n",
    "    print(f\"🔍 Routed to topic: {topic}\")\n",
    "    index_path = f\"{vector_db_folder}/index_{topic}.faiss\"\n",
    "    chunk_path = f\"{vector_db_folder}/chunks_{topic}.txt\"\n",
    "    if not os.path.exists(index_path):\n",
    "        print(\"❌ No index found for topic.\")\n",
    "        return\n",
    "\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [line.strip() for line in f.read().split(\"\\n\\n\") if line.strip()]\n",
    "\n",
    "    query_vec = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "    _, I = index.search(query_vec, k)\n",
    "    print(\"\\n📚 Top Retrieved Chunks:\")\n",
    "    for i in I[0]:\n",
    "        print(f\"\\n--- Chunk ---\\n{lines[i]}\")\n",
    "\n",
    "# 🔍 Test Example\n",
    "# search_topic_index(\"How do I recover from shoulder injury?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e4c6ae-32f4-495d-8ed2-fd075437d0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from rank_bm25) (1.26.4)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rank_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2ff9ade-dd93-412e-ab9a-6a652291f808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Indexed and saved all multi-topic FAISS indexes.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Manual Multi-Topic Chunking with Embedding and FAISS Storage\n",
    "\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Config\n",
    "input_folder = r\"C:\\Users\\Adem Tounsi\\Desktop\\FitGen\\extracted_texts\"\n",
    "vector_db_folder = \"fitgen_vector_db_topic3\"\n",
    "os.makedirs(vector_db_folder, exist_ok=True)\n",
    "\n",
    "# Use BGE base model\n",
    "model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
    "\n",
    "# Expanded keyword-to-topic mapping\n",
    "keyword_topic_map = {\n",
    "    \"shoulder\": \"injury\",\n",
    "    \"pain\": \"injury\",\n",
    "    \"injury\": \"injury\",\n",
    "    \"rehab\": \"injury\",\n",
    "    \"recovery\": \"injury\",\n",
    "    \"freestyle\": \"freestyle\",\n",
    "    \"crawl\": \"freestyle\",\n",
    "    \"kick\": \"freestyle\",\n",
    "    \"butterfly\": \"butterfly\",\n",
    "    \"dolphin\": \"butterfly\",\n",
    "    \"breaststroke\": \"breaststroke\",\n",
    "    \"frog\": \"breaststroke\",\n",
    "    \"breathing\": \"breathing\",\n",
    "    \"inhale\": \"breathing\",\n",
    "    \"exhale\": \"breathing\",\n",
    "    \"warm-up\": \"warmup\",\n",
    "    \"cooldown\": \"warmup\",\n",
    "    \"stretch\": \"warmup\",\n",
    "    \"technique\": \"technique\",\n",
    "    \"form\": \"technique\",\n",
    "    \"endurance\": \"conditioning\",\n",
    "    \"conditioning\": \"conditioning\",\n",
    "    \"strength\": \"conditioning\"\n",
    "}\n",
    "\n",
    "# Function to assign multiple topics to a chunk\n",
    "def assign_topics(text):\n",
    "    topics = set()\n",
    "    for keyword, topic in keyword_topic_map.items():\n",
    "        if keyword in text.lower():\n",
    "            topics.add(topic)\n",
    "    return topics or {\"general\"}\n",
    "\n",
    "# Chunking function\n",
    "def chunk_text(text, chunk_size=300, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        chunk = \" \".join(words[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Storage per topic\n",
    "topic_chunks = {}\n",
    "topic_sources = {}\n",
    "all_chunks = []\n",
    "all_sources = []\n",
    "\n",
    "# Process and assign chunks\n",
    "for filename in os.listdir(input_folder):\n",
    "    if not filename.endswith(\"_full_text.txt\"):\n",
    "        continue\n",
    "    path = os.path.join(input_folder, filename)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        full_text = f.read()\n",
    "    chunks = chunk_text(full_text)\n",
    "    for chunk in chunks:\n",
    "        all_chunks.append(chunk)\n",
    "        all_sources.append(filename)\n",
    "        topics = assign_topics(chunk)\n",
    "        for topic in topics:\n",
    "            topic_chunks.setdefault(topic, []).append(chunk)\n",
    "            topic_sources.setdefault(topic, []).append(filename)\n",
    "\n",
    "# Create FAISS indexes per topic\n",
    "for topic, chunks in topic_chunks.items():\n",
    "    embeddings = model.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, f\"{vector_db_folder}/index_{topic}.faiss\")\n",
    "\n",
    "    # Save chunks\n",
    "    with open(f\"{vector_db_folder}/chunks_{topic}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for source, chunk in zip(topic_sources[topic], chunks):\n",
    "            f.write(f\"### {source}\\n{chunk}\\n\\n\")\n",
    "\n",
    "# Create global BM25 index\n",
    "bm25_corpus = [chunk.split() for chunk in all_chunks]\n",
    "bm25_model = BM25Okapi(bm25_corpus)\n",
    "\n",
    "print(\"✅ Indexed and saved all multi-topic FAISS indexes.\")\n",
    "\n",
    "# Step 2: Query Router Based on Keywords and Intent\n",
    "def route_query_adaptive(query):\n",
    "    if any(word in query.lower() for word in [\"define\", \"what is\", \"list\", \"name\"]):\n",
    "        return \"bm25\"\n",
    "    elif len(query.split()) > 10:\n",
    "        return \"dense\"\n",
    "    else:\n",
    "        return \"hybrid\"\n",
    "\n",
    "# Step 3: Adaptive Search\n",
    "from sentence_transformers import util\n",
    "\n",
    "def search_adaptive(query, k=5):\n",
    "    mode = route_query_adaptive(query)\n",
    "    print(f\"🔁 Adaptive Mode: {mode}\")\n",
    "\n",
    "    results = []\n",
    "    if mode in [\"dense\", \"hybrid\"]:\n",
    "        query_vec = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "        topic = route_query(query)\n",
    "        index_path = f\"{vector_db_folder}/index_{topic}.faiss\"\n",
    "        chunk_path = f\"{vector_db_folder}/chunks_{topic}.txt\"\n",
    "        if os.path.exists(index_path):\n",
    "            index = faiss.read_index(index_path)\n",
    "            with open(chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = [line.strip() for line in f.read().split(\"\\n\\n\") if line.strip()]\n",
    "            _, I = index.search(query_vec, k)\n",
    "            results.extend([lines[i] for i in I[0]])\n",
    "\n",
    "    if mode in [\"bm25\", \"hybrid\"]:\n",
    "        tokenized_query = query.lower().split()\n",
    "        bm25_scores = bm25_model.get_scores(tokenized_query)\n",
    "        top_n = np.argsort(bm25_scores)[-k:][::-1]\n",
    "        results.extend([all_chunks[i] for i in top_n])\n",
    "\n",
    "    print(\"\\n📚 Top Retrieved Chunks:\")\n",
    "    for res in results[:k]:\n",
    "        print(f\"\\n--- Chunk ---\\n{res}\")\n",
    "\n",
    "# 🔍 Test Example\n",
    "# search_adaptive(\"List all warm-up routines before swimming\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08a9c89-e422-46ab-8232-d88520e294d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efa17701-8069-4bd3-b15b-c9b32080fcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Adaptive Mode: bm25\n",
      "\n",
      "📚 Top Retrieved Chunks:\n",
      "\n",
      "--- Chunk ---\n",
      "perform. It will also keep you from fatiguing as quickly. CONTROL It is important to control every movement against resistance, both in the power phase and recovery. Always maintain balance and stability during the power phase. Always return to the starting position gently and with less speed. Never allow an external load to jerk the limbs back to the starting position. 105 strengthtraining swimming autorin.indd 105 12.07.11 12:13 --- Page 108 Text --- 106 strengthtraining swimming autorin.indd 106 12.07.11 12:13 --- OCR from Page 108 Image 1 --- PEN SARA ANCA IN . z --- Page 109 Text --- Routines and Circuits 7 ROUTINES AND CIRCUITS This section contains strength training routines and circuits for all stages of development. Because the six stages of development identified earlier in this book are not isolated but are more of a continuum, merging one into the next, the routines and circuits are presented in four progressive levels with specific content to meet particular developmental objectives. Use these routines and circuits as examples for structure and content, then tailor them to the needs of the individual swimmer. Check with your health care provider before starting a strength training program. STRENGTH TRAINING ROUTINES The following routines are presented in a sequence beginning with a warm-up and ending with a cool down. When a double space appears, extra rest should be taken. 107 strengthtraining swimming autorin.indd 107 12.07.11 12:13 --- Page 110 Text --- Strength Training for Faster Swimming LEVEL 1 ROUTINES These routines are appropriate for athletes at the elementary stage of development and older athletes at the beginning level. 108 strengthtraining swimming autorin.indd 108 12.07.11 12:13 --- OCR from Page 110 Image 1 --- fated ee 4 sane eeen ‘rrrreress ss aia a ae at a ott om Ms sce at taceaee fs oat\n",
      "\n",
      "--- Chunk ---\n",
      "training develop better endurance, as swimming, which builds endurance very well, becomes an extension of a “super workout” begun on land, requiring the swimmer to adapt to a greater workload. On the other hand, some believe that since swimming elongates the muscles, strength work that follows swimming ensures that the swimmer’s muscles are well warmed up and less vulnerable to injury from the demands of land training. They also argue that when strength training follows the water workout, the quality of the swimming is higher because a swimmer who is tired after strength training practices swimming slower, with less force, power and explosiveness, reinforcing undesirable habits. Finally, many take the position that swimming first allows the swimmer to feel and utilize the strength that he or she has adapted to in the water where it counts. So which is best? There are valid arguments for both. From the perspective of preparing a swimmer to race, to be ready to perform under less than ideal conditions, including poor weather, crowded warm-up areas and meet delays, a program that uses a bit of both approaches should be considered. By mixing it up, the swimmer will practice adapting to different situations. Further, changing the order of the training routine from time to time will keep it from becoming boring, as the swimmer will stay challenged to perform under different circumstances. The final decision should be based on what best works for the individual athlete. 99 strengthtraining swimming autorin.indd 99 12.07.11 12:13 --- Page 102 Text --- Strength Training for Faster Swimming Scheduling rest time is as important as scheduling workout time. Even routines based solely on self-resistance work should allow at least two days off per week. When the content of the strength work includes external load, at least three rest days are\n",
      "\n",
      "--- Chunk ---\n",
      "Rope 102 strengthtraining swimming autorin.indd 102 12.07.11 12:13 --- Page 105 Text --- The Fundamentals TAKE IT EASY ON THE STRETCHING! Stretching has long been a traditional part of warming up before exercise, but the fact is, stretching has injured many swimmers. There are two reasons for this: 1. Stretching does not prepare the body to work, as a warm-up should do. By using valuable warm-up time for stretching, a swimmer runs out of time to do activities that raise the heart rate and activate the muscles. Beginning a workout in this state leaves the body more prone to injury. In addition, without a good warm-up, some or all of the workout is wasted because the swimmer is not prepared to exert. 2. In general, swimmers have very mobile joints, so stretching is really unnecessary. This is especially true of shoulder stretching. Because the shoulders are so central to swimming, they are commonly the site of soreness for many swimmers. In an effort to relieve soreness, we turn to stretching. But what happens when these muscles are stretched is that too much looseness develops, which can lead to joint stability problems during swimming, producing more soreness and pain. Instead of stretching as a warm-up, swimmers should spend warm-up time doing full-body activity with gradually progressive effort. As far as the shoulders, rather than stretching, the focus should be on stabilizing mobility at the joints to keep them operating correctly. This is done through specific shoulder stabilization exercises, as shown on page 94-95. If stretching is done at all, it should be done after exercise. It should be very conservative and target only the large muscles, not the joints. Even this kind of stretching should be gentle and gradual. Under no circumstances should any bouncing action be used. Under no circumstances\n",
      "\n",
      "--- Chunk ---\n",
      "and stabilize • Linking muscle movement • Stages of development Special section with: • Strength training routines for all levels • Strength training circuits for all levels • Shoulder maintenance routine bLyTHe LUceRo 150 mm 166 mm 166 mm 150 mm The auThor blythe Lucero has been coaching swimming for more than 25 years. She currently oversees beAR Swimming, berkeley barracudas and berkeley Aquatic Masters, where she brings her passion for swimming to the development and training of swimmers of all ages, from novice to world class. blythe grew up in berkeley, california in a large athletic family. She swam competitively in her youth, achieving All-American status in college. In addition to coaching swimming, she trains Water Safety Instructors for the Red cross and works in graphic design. “Strength Training for Faster Swimming” is her sixth book, following the three book series “coach blythe’s Swim Workouts”, the successful book “The 100 best Swimming Drills”, published in 2007, and “Masters Swimming - A Manual”, published in 2006. ISBN 978-1-84126-339-7 $ 16.95 US/£ 12.95 www.m-m-sports.com STrengTh Training for faSTer Swimming www.m-m-sports.com --- OCR from Page 172 Image 2 --- In order to enhance your performance, swimming alone is not enough. An effective strength training is crucial if you want to improve your swimming times. This book shows you what types of strength training benefit swimming and how to develop a winning routine. It includes swim- specific strength-training and lots of sample workouts. Read about: Types of strength training that benefit swimmers How to develop force, power, explosiveness and stability What makes strength training count — including transferability, adaptation, orientation, variation, over-training, season planning Using muscles to move and stabilize Linking muscle movement Stages of development Special section with: Strength training routines for all levels Strength training circuits for all levels Shoulder maintenance\n",
      "\n",
      "--- Chunk ---\n",
      "Broken Arrow drill is designed for swimmers with tight shoulders and upper backs to help them loosen off. It’s a derivative of the 6-1-6 drill and should always be performed wearing fins. --- Page 297 Text --- As in 6-1-6, kick on the side but without delay take the top hand and raise it up vertically over your head and pause it there for two seconds. This is your ‘arrow’. After pausing for a second, break the arrow by bending at the elbow and then spear into the water in front of the head. Rotate on to the other side and breathe before returning your head to the water and lifting your arm on that side. The mantra of the drill is: up - break - spear in - breathe - up - break - speak in - breathe. Whilst your arm is vertical, think about sinking your arm down into the shoulder socket for two seconds before breaking the arrow. This helps you to improve your swimming posture by engaging your scapular. When you have a feel for the rhythm of Broken Arrow, maintain a focus on your lead hand, keeping it straight and aligned by drawing your shoulders back and together. --- Page 298 Text --- Using Broken Arrow You can swim Broken Arrow at any time but as with Shoulder Tap, it’s a perfect drill to add to your warm-up to help you loosen off your shoulders. A good warm-up is: 200 m easy freestyle 300 m with fins as 3x: {50 m Broken Arrow + 50 m freestyle} 100 m easy freestyle with pull buoy --- Page 299 Text --- Popov The Popov drill is used to develop your body rotation and a classic high elbow arm recovery. The drill is named after legendary Russian sprinter\n"
     ]
    }
   ],
   "source": [
    "search_adaptive(\"List all warm-up routines before swimming\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc58eb01-eb9b-4580-a372-f95186b4965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"TOGETHER_API_KEY\"] = \"fa1509060a01b6917baaa50c6a09c4f63c385044e57153b6402599517b6d135d\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1f199a3-80d7-4d97-bd4e-4b0512f2b252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading all chunks and preparing BM25...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Ask your question (or 'exit'):  suggest for me a good workout to improve butterfly swimming\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Routed to topic: butterfly\n",
      "\n",
      "🧠 Generating with DeepSeek-V3 from Together.ai...\n",
      "\n",
      "💬 Answer:\n",
      " Here’s a concise butterfly-focused workout incorporating drills and tips from the context:  \n",
      "\n",
      "### **Butterfly Improvement Workout**  \n",
      "1. **Warm-Up (200m):**  \n",
      "   - 4x50m Freestyle (easy) with fins to loosen up.  \n",
      "\n",
      "2. **Drills (300m):**  \n",
      "   - **Undulation Drill (4x50m):** Push off, stretch streamlined, and practice chest/hip undulation (no arms). Focus on the \"down-up\" motion.  \n",
      "   - **Freestyle Arms + Butterfly Kick (4x50m):** Use fins to reinforce torso rotation and hip-driven kick. Breathe every stroke.  \n",
      "\n",
      "3. **Main Set (400\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Ask your question (or 'exit'):  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👋 Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "from together import Together\n",
    "\n",
    "# === Configuration ===\n",
    "K = 5\n",
    "CHUNK_TRUNCATE_TOKENS = 100\n",
    "MAX_TOKENS = 150\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-V3\"\n",
    "VECTOR_DB_PATH = \"fitgen_vector_db_topic3\"\n",
    "TEXT_DIR = \"fitgen_vector_db_topic3\"\n",
    "\n",
    "# === 🔑 API Key ===\n",
    "client = Together()  # Ensure TOGETHER_API_KEY is set in environment\n",
    "\n",
    "# === Embedding Model ===\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\").to(device)\n",
    "if device == \"cuda\":\n",
    "    model = model.half()\n",
    "\n",
    "# === Load all chunks and build BM25 ===\n",
    "print(\"🔄 Loading all chunks and preparing BM25...\")\n",
    "all_chunks = []\n",
    "all_sources = []\n",
    "for file in os.listdir(TEXT_DIR):\n",
    "    if file.startswith(\"chunks_\") and file.endswith(\".txt\"):\n",
    "        topic = file[len(\"chunks_\"):-len(\".txt\")]\n",
    "        with open(os.path.join(TEXT_DIR, file), encoding=\"utf-8\") as f:\n",
    "            entries = [line.strip() for line in f.read().split(\"\\n\\n\") if line.strip()]\n",
    "            all_chunks.extend(entries)\n",
    "            all_sources.extend([topic] * len(entries))\n",
    "\n",
    "bm25_model = BM25Okapi([chunk.split() for chunk in all_chunks])\n",
    "\n",
    "# === Topic routing ===\n",
    "keyword_topic_map = {\n",
    "    \"shoulder\": \"injury\", \"pain\": \"injury\", \"rehab\": \"injury\",\n",
    "    \"freestyle\": \"freestyle\", \"crawl\": \"freestyle\", \"kick\": \"freestyle\",\n",
    "    \"butterfly\": \"butterfly\", \"dolphin\": \"butterfly\",\n",
    "    \"breaststroke\": \"breaststroke\", \"frog\": \"breaststroke\",\n",
    "    \"breathing\": \"breathing\", \"inhale\": \"breathing\", \"exhale\": \"breathing\",\n",
    "    \"warm-up\": \"warmup\", \"cooldown\": \"warmup\", \"stretch\": \"warmup\",\n",
    "    \"technique\": \"technique\", \"form\": \"technique\",\n",
    "    \"endurance\": \"conditioning\", \"strength\": \"conditioning\"\n",
    "}\n",
    "\n",
    "def route_topic(query):\n",
    "    for keyword, topic in keyword_topic_map.items():\n",
    "        if keyword in query.lower():\n",
    "            return topic\n",
    "    return \"general\"\n",
    "\n",
    "def truncate(text, max_tokens=100):\n",
    "    return ' '.join(text.split()[:max_tokens])\n",
    "\n",
    "# === Adaptive RAG loop ===\n",
    "while True:\n",
    "    query = input(\"\\n🔍 Ask your question (or 'exit'): \").strip()\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "\n",
    "    topic = route_topic(query)\n",
    "    print(f\"🔁 Routed to topic: {topic}\")\n",
    "\n",
    "    # --- FAISS search ---\n",
    "    dense_chunks = []\n",
    "    faiss_index_path = f\"{VECTOR_DB_PATH}/index_{topic}.faiss\"\n",
    "    chunk_path = f\"{VECTOR_DB_PATH}/chunks_{topic}.txt\"\n",
    "    if os.path.exists(faiss_index_path):\n",
    "        index = faiss.read_index(faiss_index_path)\n",
    "        with open(chunk_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            topic_chunks = [line.strip() for line in f.read().split(\"\\n\\n\") if line.strip()]\n",
    "        query_vec = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
    "        _, I = index.search(query_vec, k=K)\n",
    "        dense_chunks = [truncate(topic_chunks[i], CHUNK_TRUNCATE_TOKENS) for i in I[0]]\n",
    "\n",
    "    # --- BM25 search ---\n",
    "    tokenized_query = query.lower().split()\n",
    "    bm25_scores = bm25_model.get_scores(tokenized_query)\n",
    "    top_n = np.argsort(bm25_scores)[-K:][::-1]\n",
    "    bm25_chunks = [truncate(all_chunks[i], CHUNK_TRUNCATE_TOKENS) for i in top_n]\n",
    "\n",
    "    # --- Combine results ---\n",
    "    all_results = list(dict.fromkeys(dense_chunks + bm25_chunks))[:K]\n",
    "    context = \"\\n\\n\".join(all_results)\n",
    "\n",
    "    # --- Dynamic prompt ---\n",
    "    if query.lower().startswith(\"define\") or len(query.split()) < 5:\n",
    "        prompt = f\"Answer this directly and concisely:\\n{query}\"\n",
    "    else:\n",
    "        prompt = f\"\"\"You are a helpful assistant.\n",
    "\n",
    "Use the following context to answer the user's question concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    print(\"\\n🧠 Generating with DeepSeek-V3 from Together.ai...\")\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        print(\"\\n💬 Answer:\\n\", response.choices[0].message.content.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error: {e}\")\n",
    "\n",
    "print(\"👋 Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
